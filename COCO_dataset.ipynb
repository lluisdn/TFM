{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lluisdn/TFM/blob/YOLO/COCO_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVZFGU2gOEMT"
      },
      "source": [
        "#**5000 Images COCO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg7NDwYnODjW",
        "outputId": "ca580d7a-a9c6-4373-e8a5-93e5dc78377d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', '-r', 'requirements.txt'], returncode=0)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "\n",
        "# Clone YOLOv5 repository\n",
        "if not os.path.exists('yolov5'):\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/ultralytics/yolov5.git'])\n",
        "\n",
        "\n",
        "# Change working directory to yolov5 directory\n",
        "os.chdir('yolov5')\n",
        "\n",
        "# Install required packages\n",
        "subprocess.run(['pip', 'install', '-r', 'requirements.txt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVCR_dOIlD9o",
        "outputId": "69db31eb-19ca-4e85-ab5d-32cc32e32509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fiftyone\n",
            "  Downloading fiftyone-0.21.0-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from fiftyone)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting argcomplete (from fiftyone)\n",
            "  Downloading argcomplete-3.1.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from fiftyone)\n",
            "  Downloading boto3-1.26.160-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.3.1)\n",
            "Collecting dacite<1.8.0,>=1.6.0 (from fiftyone)\n",
            "  Downloading dacite-1.7.0-py3-none-any.whl (12 kB)\n",
            "Collecting Deprecated (from fiftyone)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting eventlet (from fiftyone)\n",
            "  Downloading eventlet-0.33.3-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy (from fiftyone)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.18.3)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone)\n",
            "  Downloading Hypercorn-0.14.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.2)\n",
            "Collecting kaleido (from fiftyone)\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.7.1)\n",
            "Collecting mongoengine==0.24.2 (from fiftyone)\n",
            "  Downloading mongoengine-0.24.2-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting motor>=2.5 (from fiftyone)\n",
            "  Downloading motor-3.2.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fiftyone) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.5.3)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (8.4.0)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.13.1)\n",
            "Collecting pprintpp (from fiftyone)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.9.5)\n",
            "Collecting pymongo>=3.12 (from fiftyone)\n",
            "  Downloading pymongo-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (648 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.9/648.9 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2022.7.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2022.10.31)\n",
            "Collecting retrying (from fiftyone)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.19.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (67.7.2)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n",
            "  Downloading sseclient_py-1.7.2-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Collecting starlette<0.27,>=0.24.0 (from fiftyone)\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting strawberry-graphql==0.138.1 (from fiftyone)\n",
            "  Downloading strawberry_graphql-0.138.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.8.10)\n",
            "Collecting xmltodict (from fiftyone)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting fiftyone-brain<0.13,>=0.12 (from fiftyone)\n",
            "  Downloading fiftyone_brain-0.12.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fiftyone-db<0.5,>=0.4 (from fiftyone)\n",
            "  Downloading fiftyone_db-0.4.0-py3-none-manylinux1_x86_64.whl (37.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting voxel51-eta<0.11,>=0.10 (from fiftyone)\n",
            "  Downloading voxel51_eta-0.10.0-py2.py3-none-any.whl (568 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.9/568.9 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.7.0.72)\n",
            "Collecting graphql-core<3.3.0,>=3.2.0 (from strawberry-graphql==0.138.1->fiftyone)\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (4.6.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone-brain<0.13,>=0.12->fiftyone) (1.10.1)\n",
            "Collecting h11 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2>=3.1.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting priority (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.10.2)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3->fiftyone) (2.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.14->fiftyone) (8.2.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo>=3.12->fiftyone)\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.27,>=0.24.0->fiftyone) (3.7.0)\n",
            "Collecting httpx>=0.10.0 (from universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.11,>=0.10->fiftyone) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rarfile (from voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.11,>=0.10->fiftyone) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.11,>=0.10->fiftyone) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.11,>=0.10->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.11,>=0.10->fiftyone) (5.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.11,>=0.10->fiftyone) (1.26.16)\n",
            "Collecting botocore<1.30.0,>=1.29.160 (from boto3->fiftyone)\n",
            "  Downloading botocore-1.29.160-py3-none-any.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->fiftyone)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->fiftyone) (1.14.1)\n",
            "Requirement already satisfied: greenlet>=0.3 in /usr/local/lib/python3.10/dist-packages (from eventlet->fiftyone) (2.0.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->fiftyone) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (3.1.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (3.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.27,>=0.24.0->fiftyone) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.27,>=0.24.0->fiftyone) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.27,>=0.24.0->fiftyone) (1.1.1)\n",
            "Collecting hyperframe<7,>=6.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2023.5.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->voxel51-eta<0.11,>=0.10->fiftyone) (23.1.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6 (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting brotli>=1.0.9 (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr->voxel51-eta<0.11,>=0.10->fiftyone)\n",
            "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->voxel51-eta<0.11,>=0.10->fiftyone) (2.0.12)\n",
            "Installing collected packages: texttable, sseclient-py, rarfile, pprintpp, kaleido, brotli, xmltodict, retrying, pyzstd, pyppmd, pycryptodomex, pybcj, priority, multivolumefile, jsonlines, jmespath, inflate64, hyperframe, hpack, h11, graphql-core, ftfy, fiftyone-db, dnspython, dill, Deprecated, dacite, argcomplete, aiofiles, wsproto, strawberry-graphql, starlette, pymongo, py7zr, httpcore, h2, eventlet, botocore, voxel51-eta, sse-starlette, s3transfer, motor, mongoengine, hypercorn, httpx, fiftyone-brain, universal-analytics-python3, boto3, fiftyone\n",
            "Successfully installed Deprecated-1.2.14 aiofiles-23.1.0 argcomplete-3.1.1 boto3-1.26.160 botocore-1.29.160 brotli-1.0.9 dacite-1.7.0 dill-0.3.6 dnspython-2.3.0 eventlet-0.33.3 fiftyone-0.21.0 fiftyone-brain-0.12.0 fiftyone-db-0.4.0 ftfy-6.1.1 graphql-core-3.2.3 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.17.2 httpx-0.24.1 hypercorn-0.14.3 hyperframe-6.0.1 inflate64-0.3.1 jmespath-1.0.1 jsonlines-3.1.0 kaleido-0.2.1 mongoengine-0.24.2 motor-3.2.0 multivolumefile-0.2.3 pprintpp-0.4.0 priority-2.0.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pymongo-4.4.0 pyppmd-1.0.0 pyzstd-0.15.9 rarfile-4.0 retrying-1.3.4 s3transfer-0.6.1 sse-starlette-0.10.3 sseclient-py-1.7.2 starlette-0.26.1 strawberry-graphql-0.138.1 texttable-1.6.7 universal-analytics-python3-1.1.1 voxel51-eta-0.10.0 wsproto-1.2.0 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k0Sp9Zok533",
        "outputId": "44710c4a-58da-40ec-dc66-7c07e58213a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Migrating database to v0.21.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating database to v0.21.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |██████|    1.9Gb/1.9Gb [8.3s elapsed, 0s remaining, 259.5Mb/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    1.9Gb/1.9Gb [8.3s elapsed, 0s remaining, 259.5Mb/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |██████|    6.1Gb/6.1Gb [24.5s elapsed, 0s remaining, 259.2Mb/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    6.1Gb/6.1Gb [24.5s elapsed, 0s remaining, 259.2Mb/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |███████████████| 5000/5000 [38.0s elapsed, 0s remaining, 167.0 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 5000/5000 [38.0s elapsed, 0s remaining, 167.0 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'coco-2017-validation' created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'coco-2017-validation' created\n"
          ]
        }
      ],
      "source": [
        "import fiftyone.zoo as foz\n",
        "\n",
        "# To download the COCO dataset for only the \"person\" and \"car\" classes\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"]#, \"segmentations\"]#,\n",
        "    #classes=[\"person\", \"car\"],\n",
        "    # max_samples=50,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zyS_yyWwIMOH",
        "outputId": "f303d707-fa53-4851-9e4e-89401ab9755e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_f808bff1-d179-44bc-98f7-669bfe32fd37\", \"validation_txt.zip\", 1476958)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Define the path of the folder to download\n",
        "folder_path = '/content/validation_txt'\n",
        "\n",
        "# Create a zip file of the folder\n",
        "shutil.make_archive(folder_path, 'zip', folder_path)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(folder_path + '.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SV17ZQ1mmD91",
        "outputId": "2e276b4c-481d-4030-e6f2-e0d82d005afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name:        coco-2017-validation\n",
            "Media type:  image\n",
            "Num samples: 5000\n",
            "Persistent:  False\n",
            "Tags:        []\n",
            "Sample fields:\n",
            "    id:           fiftyone.core.fields.ObjectIdField\n",
            "    filepath:     fiftyone.core.fields.StringField\n",
            "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
            "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
            "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LNkeC6zTt7wO",
        "outputId": "64967f75-bf73-4579-83f9-4338d62e1023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# COCO dataset label order\n",
        "coco_labels = [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
        "    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
        "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "    'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
        "    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
        "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# Create a mapping dictionary\n",
        "label_to_number = {label: number for number, label in enumerate(coco_labels, start=0)}\n",
        "\n",
        "\n",
        "number = label_to_number.get('car', 100)  # Get the corresponding number, default to 0 if label is not found\n",
        "\n",
        "print(number)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqHl00h_5SKr"
      },
      "outputs": [],
      "source": [
        "common_path='/content/'\n",
        "#print(common_path,image_filename)\n",
        "image_filename='000000000139.txt'\n",
        "file_path = os.path.join(common_path, image_filename)\n",
        "os.remove(file_path)\n",
        "#os.remove('/content/000000000139.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yo-LTrVX_LVg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "folder_name = '/content/validation_txt'\n",
        "os.makedirs(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CaAZM6hEce6"
      },
      "outputs": [],
      "source": [
        "common_path = '/content/validation_txt'\n",
        "import os\n",
        "\n",
        "for sample in dataset:\n",
        "    image_filename = os.path.basename(sample.filename)\n",
        "    text_file_path = image_filename.replace('.jpg', '.txt')\n",
        "    save_path_file = os.path.join(common_path, text_file_path)\n",
        "    print(save_path_file)\n",
        "    #if os.path.exists(path_delete_file):\n",
        "    #  os.remove(path_delete_file)\n",
        "    #print(sample.ground_truth.detections)\n",
        "    if sample.ground_truth is not None:#.detections is not None:\n",
        "      sdd = sample.ground_truth.detections\n",
        "      ww=[]\n",
        "      for detection in sdd:\n",
        "          label = detection.label\n",
        "          number = label_to_number.get(label, 100)\n",
        "          bounding_box = detection.bounding_box\n",
        "          line = f\"{number} {bounding_box[0]} {bounding_box[1]} {bounding_box[2]} {bounding_box[3]}\"\n",
        "          ww.append(line)\n",
        "      with open(save_path_file, \"w\") as file:\n",
        "          for element in ww:\n",
        "            file.write(element + \"\\n\")\n",
        "          file.close()\n",
        "      ww=[]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2KsBYmm_qlE",
        "outputId": "9a2f9719-30a7-44a0-af60-2b8084801942"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/000000000776.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Do something with the file content\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "ULq9xu6NmzKj",
        "outputId": "69d9b610-a723-4697-b729-9287c4648102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content/validation_txt 000000000776.jpg\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fiftyone/core/document.py\u001b[0m in \u001b[0;36mget_field\u001b[0;34m(self, field_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fiftyone/core/odm/mixins.py\u001b[0m in \u001b[0;36mget_field\u001b[0;34m(self, field_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;34m\"%s has no field '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_doc_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Sample has no field 'detections'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-99fefaf94c50>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimage_filename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'000000000776.jpg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0;31m#print(sample.detections.detections)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0msdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fiftyone/core/sample.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fiftyone/core/document.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fiftyone/core/sample.py\u001b[0m in \u001b[0;36mget_field\u001b[0;34m(self, field_name)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     def set_field(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fiftyone/core/document.py\u001b[0m in \u001b[0;36mget_field\u001b[0;34m(self, field_name)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0;34m\"%s has no field '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: Sample has no field 'detections'"
          ]
        }
      ],
      "source": [
        "# Iterate over samples\n",
        "common_path='content/validation_txt'\n",
        "\n",
        "for sample in dataset:\n",
        "    #print(sample.filename)\n",
        "    image_filename = os.path.basename(sample.filename)\n",
        "    text_file_path = image_filename.replace('.jpg', '.txt')\n",
        "    path_delete_file = os.path.join(common_path, image_filename)\n",
        "    #if os.path.exists(path_delete_file):\n",
        "    #  os.remove(path_delete_file)\n",
        "\n",
        "\n",
        "    if image_filename == '000000000776.jpg':\n",
        "      print(common_path,image_filename)\n",
        "      print(len(sample.detections.detections))\n",
        "      #print(sample.detections.detections)\n",
        "      sdd=sample.detections.detections\n",
        "      #print(sdd)\n",
        "      with open(text_file_path, 'w') as text_file:\n",
        "        #for _ in range(len(sample.detections.detections)):\n",
        "        for i in sdd:\n",
        "          #print(i)\n",
        "          label = i.label\n",
        "          number = label_to_number.get(label, 100)\n",
        "          bounding_box = i.bounding_box\n",
        "          print(i.label)\n",
        "          print(label_to_number.get(i.label, 100))\n",
        "          print(i.bounding_box)\n",
        "\n",
        "              #text_file.write(f\"{number} {bounding_box[0]} {bounding_box[1]} {bounding_box[2]} {bounding_box[3]}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      with open(text_file_path, 'w') as text_file:\n",
        "        for detect in sample.detections.detections:\n",
        "            label = detect.label\n",
        "            number = label_to_number.get(label, 100)\n",
        "            bounding_box = detect.bounding_box\n",
        "            #for _ in range(len(sample.detections.detections)):\n",
        "            text_file.write(f\"{number} {bounding_box[0]} {bounding_box[1]} {bounding_box[2]} {bounding_box[3]}\\n\")\n",
        "\n",
        "    \"\"\"\n",
        "    if sample.detections is not None:\n",
        "      with open(text_file_path, 'w') as text_file:\n",
        "        if sample.detections.detections is not None:\n",
        "          print(len(sample.detections.detections))\n",
        "          for detect in sample.detections.detections:\n",
        "            label = detect.label\n",
        "            number = label_to_number.get(label, 100)\n",
        "            bounding_box = detect.bounding_box\n",
        "            for _ in range(len(sample.detections.detections)):\n",
        "              text_file.write(f\"{number} {bounding_box[0]} {bounding_box[1]} {bounding_box[2]} {bounding_box[3]}\\n\")\n",
        "    \"\"\"\n",
        "\n",
        "    #text_file_path = image_filename.replace('.jpg', '.txt')\n",
        "    #with open(text_file_path, 'w') as text_file:\n",
        "      #detections = sample.detections\n",
        "      #print(detections)\n",
        "    #if sample.detections is not None:\n",
        "    #  for detection in sample.detections:\n",
        "    #    if image_filename == '000000000139.jpg':\n",
        "    #        for detection1 in detection:\n",
        "    #            print(detection1)\n",
        "            #label = detection.label\n",
        "            #number = label_to_number.get(label, 100)\n",
        "            #bounding_box = detection.bounding_box\n",
        "            #text_file.write(f\"{number} {bounding_box[0]} {bounding_box[1]} {bounding_box[2]} {bounding_box[3]}\")\n",
        "\n",
        "\n",
        "#print(f\"Created {text_file_path} with label '{label}' and bounding box {bounding_box}\")\n",
        "#print(label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bl9RlGHBdXj"
      },
      "outputs": [],
      "source": [
        "file_path = \"content/proba.txt\"\n",
        "new_elements = [\"element1\", \"element2\", \"element3\"]\n",
        "\n",
        "with open(file_path, \"a\") as file:\n",
        "    for element in new_elements:\n",
        "        file.write(element + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVq9WtmjBpCt",
        "outputId": "f898bf03-8f18-4443-d0b9-1d6529aa32f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "element1\n",
            "element2\n",
            "element3\n",
            "element1\n",
            "element2\n",
            "element3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = \"content/proba.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Do something with the file content\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-RGuLt6PopQ"
      },
      "source": [
        "#**try**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QWKLCFEQ_MK"
      },
      "source": [
        "##**Functions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJmUsm7CQ7tP"
      },
      "outputs": [],
      "source": [
        "from utils.general import (xywh2xyxy, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
        "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)#,non_max_suppression2)\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "def non_max_suppression2(\n",
        "        prediction,\n",
        "        conf_thres=0.25,\n",
        "        iou_thres=0.45,\n",
        "        classes=None,\n",
        "        agnostic=False,\n",
        "        multi_label=False,\n",
        "        labels=(),\n",
        "        max_det=300,\n",
        "        nm=0,  # number of masks\n",
        "):\n",
        "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
        "\n",
        "    Returns:\n",
        "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
        "    \"\"\"\n",
        "\n",
        "    # Checks\n",
        "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "    if isinstance(prediction, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
        "        prediction = prediction[0]  # select only inference output\n",
        "\n",
        "    device = prediction.device\n",
        "    mps = 'mps' in device.type  # Apple MPS\n",
        "    if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\n",
        "        prediction = prediction.cpu()\n",
        "    bs = prediction.shape[0]  # batch size\n",
        "    nc = prediction.shape[2] - nm - 5  # number of classes\n",
        "    xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "    # Settings\n",
        "    # min_wh = 2  # (pixels) minimum box width and height\n",
        "    max_wh = 7680  # (pixels) maximum box width and height\n",
        "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
        "    redundant = True  # require redundant detections\n",
        "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "    merge = False  # use merge-NMS\n",
        "\n",
        "    #t = time.time()\n",
        "    mi = 5 + nc  # mask start index\n",
        "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
        "    output2 = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
        "    for xi, x in enumerate(prediction):  # image index, image inference\n",
        "        # Apply constraints\n",
        "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "        x = x[xc[xi]]  # confidence\n",
        "        x_info=x.clone()\n",
        "\n",
        "        # Cat apriori labels if autolabelling\n",
        "        if labels and len(labels[xi]):\n",
        "            lb = labels[xi]\n",
        "            v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n",
        "            v[:, :4] = lb[:, 1:5]  # box\n",
        "            v[:, 4] = 1.0  # conf\n",
        "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
        "            x = torch.cat((x, v), 0)\n",
        "\n",
        "        # If none remain process next image\n",
        "        if not x.shape[0]:\n",
        "            continue\n",
        "\n",
        "        # Compute conf\n",
        "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "        # Box/Mask\n",
        "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
        "        mask = x[:, mi:]  # zero columns if no masks\n",
        "\n",
        "        # Detections matrix nx6 (xyxy, conf, cls)\n",
        "        if multi_label:\n",
        "            i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
        "            x = torch.cat((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
        "        else:  # best class only\n",
        "            conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
        "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
        "\n",
        "        # Filter by class\n",
        "        if classes is not None:\n",
        "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "        # Apply finite constraint\n",
        "        # if not torch.isfinite(x).all():\n",
        "        #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "        # Check shape\n",
        "        n = x.shape[0]  # number of boxes\n",
        "        if not n:  # no boxes\n",
        "            continue\n",
        "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
        "\n",
        "        # Batched NMS\n",
        "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "        i = i[:max_det]  # limit detections\n",
        "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "            weights = iou * scores[None]  # box weights\n",
        "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "            if redundant:\n",
        "                i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "        output[xi] = x[i]\n",
        "        output2[xi]= x_info[i]\n",
        "        if mps:\n",
        "            output[xi] = output[xi].to(device)\n",
        "            output2[xi] = output2[xi].to(device)\n",
        "\n",
        "        #if (time.time() - t) > time_limit:\n",
        "        #    LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
        "        #    break  # time limit exceeded\n",
        "\n",
        "    return output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-4u8pRRPrLV"
      },
      "outputs": [],
      "source": [
        "#source= '/content/000000000009.jpg'#'/content/image.jpg'\n",
        "def detection_probs(source, visualize=False):\n",
        "    save_dir_2 ='/content/solus'\n",
        "    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())\n",
        "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        with dt[0]:\n",
        "            im = torch.from_numpy(im).to(model.device)\n",
        "            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "            if len(im.shape) == 3:\n",
        "                im = im[None]  # expand for batch dim\n",
        "      # Inference\n",
        "        with dt[1]:\n",
        "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "            pred = model(im, augment=augment, visualize=visualize)\n",
        "            pred2 = pred.copy()\n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            #print(pred2)\n",
        "            print('pred2',[np.array(i).shape for i in pred2] )#[np.array(i).shape for i in y],-->[np.shape(i) for i in y]\n",
        "            if isinstance(pred2, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
        "              pred2 = pred2[0]  # select only inference output\n",
        "            print(pred2.shape)\n",
        "\n",
        "    #device = prediction.device\n",
        "    #mps = 'mps' in device.type  # Apple MPS\n",
        "    #if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\n",
        "    #    prediction = prediction.cpu()\n",
        "    #bs = prediction.shape[0]  # batch size\n",
        "    #nc = prediction.shape[2] - nm - 5  # number of classes\n",
        "            xc = pred2[..., 4] > conf_thres  # candidates\n",
        "            print(xc)\n",
        "            for xi, x in enumerate(pred2):  # image index, image inference\n",
        "              print(xi)\n",
        "              print(x.shape)\n",
        "        # Apply constraints\n",
        "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "              x = x[xc[xi]]  # confidence\n",
        "\n",
        "            print(x.shape)\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "        # NMS\n",
        "        with dt[2]:\n",
        "            probs = non_max_suppression2(pred2, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "            print('shape probs:',probs[0].shape)\n",
        "            for Ldetection in probs[0]:\n",
        "              print(Ldetection[5:].sum(axis=-1))\n",
        "              #print(probs[0][Ldetection][5:].sum(axis=-1))\n",
        "            #print(conf.sum(axis=-1))\n",
        "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "    # Second-stage classifier (optional)\n",
        "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            seen += 1\n",
        "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # im.jpg\n",
        "\n",
        "            txt_path = Path(save_dir_2)#str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
        "            txt_path = str(txt_path / (p.stem + '.txt'))\n",
        "            if os.path.exists(txt_path):\n",
        "              os.remove(txt_path)  # Delete the existing file\n",
        "\n",
        "            s += '%gx%g ' % im.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "                print(\"Rescale boxes from img_size to im0 size\")\n",
        "                print(det[:,:4])\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, 5].unique():\n",
        "                    n = (det[:, 5] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to stringç\n",
        "                print(\" Print results\")\n",
        "                print(n)\n",
        "                print(s)\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        print(xywh, int(cls))\n",
        "\n",
        "                        with open(f'{txt_path}', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
        "\n",
        "                    if save_crop:\n",
        "                        save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
        "\n",
        "            # Stream results\n",
        "            im0 = annotator.result()\n",
        "            if view_img:\n",
        "                if platform.system() == 'Linux' and p not in windows:\n",
        "                    windows.append(p)\n",
        "                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
        "                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path[i] != save_path:  # new video\n",
        "                        vid_path[i] = save_path\n",
        "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
        "                            vid_writer[i].release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
        "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer[i].write(im0)\n",
        "\n",
        "        # Print time (inference-only)\n",
        "        LOGGER.info(f\"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms\")\n",
        "\n",
        "    # Print results\n",
        "    t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image\n",
        "    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
        "    if update:\n",
        "        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n",
        "    for Ldetection in probs[0]:\n",
        "      print(Ldetection[5:].sum(axis=-1))\n",
        "\n",
        "    return probs, xywh, int(cls), txt_path, probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d6nG28ZRFxh"
      },
      "source": [
        "##**Folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtYgrLPWQZXl"
      },
      "outputs": [],
      "source": [
        "folder_name = '/content/solus'\n",
        "os.makedirs(folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URJVNnYwRInG"
      },
      "source": [
        "##**Pre automatic detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAixox2kQsZa",
        "outputId": "eef033e9-03e6-4307-94f1-9ceb33899133"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 v7.0-185-g2334aa7 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.lib.display import exists\n",
        "import argparse\n",
        "import os\n",
        "import platform\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "#from models.common import DetectMultiBackend######!!!!! COMENTAR PER A MODEL\n",
        "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
        "from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
        "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh, check_requirements, check_suffix)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "weights=['yolov5s.pt']\n",
        "half=False\n",
        "device=''\n",
        "device = select_device(device)\n",
        "dnn=False\n",
        "data = 'data/coco128.yaml'\n",
        "#data = '/content/000000000034.jpg'\n",
        "imgsz=(640, 640)\n",
        "source= '/content/000000000009.jpg'#/content/000000000009.jpg'#'/content/image.jpg'\n",
        "project='/content/yolov5/runs/detect'\n",
        "vid_stride=1\n",
        "visualize=False\n",
        "augment=False\n",
        "conf_thres=0.25\n",
        "iou_thres=0.25\n",
        "classes=None\n",
        "agnostic_nms=False\n",
        "max_det=1000\n",
        "name='exp'\n",
        "exist_ok=False\n",
        "save_txt=True\n",
        "save_crop=False\n",
        "view_img=False\n",
        "update= False\n",
        "save_conf=True\n",
        "nosave=False\n",
        "line_thickness=3\n",
        "hide_labels=False,  # hide labels\n",
        "hide_conf=False,  # hide confidences\n",
        "webcam=False,\n",
        "\n",
        "save_img = not nosave and not source.endswith('.txt')  # save inference images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_PEViyQTNkt",
        "outputId": "f957f0b0-5188-4547-9b04-c1f6cf980cd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 v7.0-185-g2334aa7 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.lib.display import exists\n",
        "import argparse\n",
        "import os\n",
        "import platform\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from models.common import DetectMultiBackend######!!!!! COMENTAR PER A MODEL\n",
        "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
        "from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
        "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh, check_requirements, check_suffix)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "weights=['yolov5s.pt']\n",
        "half=False\n",
        "device=''\n",
        "device = select_device(device)\n",
        "dnn=False\n",
        "data = 'data/coco128.yaml'\n",
        "#data = '/content/000000000034.jpg'\n",
        "imgsz=(640, 640)\n",
        "source= '/content/000000000009.jpg'#/content/000000000009.jpg'#'/content/image.jpg'\n",
        "project='/content/yolov5/runs/detect'\n",
        "vid_stride=1\n",
        "visualize=False\n",
        "augment=False\n",
        "conf_thres=0.25\n",
        "iou_thres=0.25\n",
        "classes=None\n",
        "agnostic_nms=False\n",
        "max_det=1000\n",
        "name='exp'\n",
        "exist_ok=False\n",
        "save_txt=True\n",
        "save_crop=False\n",
        "view_img=False\n",
        "update= False\n",
        "save_conf=True\n",
        "nosave=False\n",
        "line_thickness=3\n",
        "hide_labels=False,  # hide labels\n",
        "hide_conf=False,  # hide confidences\n",
        "webcam=False,\n",
        "\n",
        "save_img = not nosave and not source.endswith('.txt')  # save inference images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWJ04Rk0RNcz"
      },
      "source": [
        "##**Automatic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RlELm1ATF8C",
        "outputId": "2bcf9b3c-a357-4474-d357-59a2e917a0a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov5/runs/detect/exp3\n",
            "direcion\n",
            "/content/solus\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: <utils.dataloaders.LoadImages object at 0x7f2437c4c580>\n",
            "path: /content/000000000009.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 /content/000000000009.jpg: 480x640 2 bowls, 2 broccolis, 1 dining table, 364.2ms\n",
            "Speed: 7.1ms pre-process, 364.2ms inference, 120.6ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/yolov5/runs/detect/exp3\u001b[0m\n",
            "0 labels saved to /content/yolov5/runs/detect/exp3/labels\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.01276)\n",
            "tensor(1.34580)\n",
            "tensor(1.01389)\n",
            "tensor(1.00977)\n",
            "tensor(1.02566)\n",
            "/content/yolov5/runs/detect/exp3\n",
            "Rescale boxes from img_size to im0 size\n",
            "tensor([[339.,   0., 629., 239.],\n",
            "        [256., 227., 573., 480.],\n",
            "        [250., 222., 572., 477.],\n",
            "        [327., 411., 427., 480.],\n",
            "        [  0.,   6., 623., 480.]])\n",
            " Print results\n",
            "tensor(1)\n",
            "image 1/1 /content/000000000009.jpg: 480x640 2 bowls, 2 broccolis, 1 dining table, \n",
            "[0.4867187440395355, 0.5062500238418579, 0.973437488079071, 0.987500011920929] 60\n",
            "[0.589062511920929, 0.9281250238418579, 0.15625, 0.14374999701976776] 50\n",
            "[0.6421874761581421, 0.7281249761581421, 0.503125011920929, 0.53125] 45\n",
            "[0.647656261920929, 0.7364583611488342, 0.49531251192092896, 0.5270833373069763] 50\n",
            "[0.7562500238418579, 0.24895833432674408, 0.453125, 0.49791666865348816] 45\n"
          ]
        }
      ],
      "source": [
        "save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)\n",
        "print(save_dir)\n",
        "(save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)\n",
        "save_dir_2 ='/content/solus'\n",
        "print(\"direcion\")\n",
        "print(save_dir_2)\n",
        "\n",
        "model = DetectMultiBackend(weights , device=device, dnn=dnn, data=data, fp16=half)\n",
        "stride, names, pt = model.stride, model.names, model.pt\n",
        "imgsz = check_img_size(imgsz, s=stride)\n",
        "\n",
        "# Dataloader\n",
        "bs = 1  # batch_size\n",
        "dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
        "vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "print('dataset:', dataset)\n",
        "\n",
        "# Run inference\n",
        "model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
        "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())\n",
        "for path, im, im0s, vid_cap, s in dataset:\n",
        "    print('path:',path)\n",
        "    with dt[0]:\n",
        "        im = torch.from_numpy(im).to(model.device)\n",
        "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "  # Inference\n",
        "    with dt[1]:\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize)\n",
        "        #print(model)\n",
        "        pred2 = pred.copy()\n",
        "    # NMS\n",
        "    with dt[2]:\n",
        "        probs = non_max_suppression2(pred2, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        for Ldetection in probs[0]:\n",
        "            print(Ldetection[5:].sum(axis=-1))\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "\n",
        "    # Second-stage classifier (optional)\n",
        "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            seen += 1\n",
        "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # im.jpg\n",
        "            print(save_dir)\n",
        "            #txt_path =   p.stem\n",
        "            #txt_path = str(save_dir_2 / p.stem ) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
        "            #str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
        "            solus_directory = Path(save_dir_2)   # Create a subdirectory using p.stem as the directory name\n",
        "            #solus_directory.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "            solus_txt_file_path = str(solus_directory / (p.stem + '.txt'))\n",
        "\n",
        "\n",
        "            s += '%gx%g ' % im.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "                print(\"Rescale boxes from img_size to im0 size\")\n",
        "                print(det[:,:4])\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, 5].unique():\n",
        "                    n = (det[:, 5] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to stringç\n",
        "                print(\" Print results\")\n",
        "                print(n)\n",
        "                print(s)\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        print(xywh, int(cls))\n",
        "\n",
        "                        with open(solus_txt_file_path, 'a') as f:\n",
        "                        #with open(f'{txt_path}.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
        "\n",
        "                    if save_crop:\n",
        "                        save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
        "\n",
        "            # Stream results\n",
        "            im0 = annotator.result()\n",
        "            if view_img:\n",
        "                if platform.system() == 'Linux' and p not in windows:\n",
        "                    windows.append(p)\n",
        "                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
        "                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path[i] != save_path:  # new video\n",
        "                        vid_path[i] = save_path\n",
        "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
        "                            vid_writer[i].release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
        "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer[i].write(im0)\n",
        "\n",
        "        # Print time (inference-only)\n",
        "        LOGGER.info(f\"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms\")\n",
        "\n",
        "    # Print results\n",
        "    t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image\n",
        "    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
        "    if update:\n",
        "        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE--2MXxPy8P",
        "outputId": "2eb9d335-ac28-4cce-aeec-8e53362c5497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of images is: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 /content/000000000139.jpg: 448x640 1 person, 1 bottle, 3 chairs, 2 potted plants, 3 dining tables, 2 tvs, 2 refrigerators, 1 clock, 4 vases, 294.5ms\n",
            "Speed: 1.3ms pre-process, 294.5ms inference, 22.1ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/yolov5/runs/detect/exp3\u001b[0m\n",
            "0 labels saved to /content/yolov5/runs/detect/exp3/labels\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape probs: torch.Size([19, 85])\n",
            "tensor(1.01037)\n",
            "tensor(1.03542)\n",
            "tensor(1.00959)\n",
            "tensor(0.99942)\n",
            "tensor(1.00796)\n",
            "tensor(1.00335)\n",
            "tensor(1.28844)\n",
            "tensor(1.00858)\n",
            "tensor(0.86293)\n",
            "tensor(0.92262)\n",
            "tensor(1.00293)\n",
            "tensor(1.00843)\n",
            "tensor(1.01640)\n",
            "tensor(1.01836)\n",
            "tensor(1.01693)\n",
            "tensor(1.02363)\n",
            "tensor(1.00384)\n",
            "tensor(1.12427)\n",
            "tensor(0.96724)\n",
            "Rescale boxes from img_size to im0 size\n",
            "tensor([[  6., 166., 154., 264.],\n",
            "        [293., 218., 354., 318.],\n",
            "        [413., 155., 466., 299.],\n",
            "        [558., 212., 640., 283.],\n",
            "        [449., 168., 514., 291.],\n",
            "        [550., 301., 586., 399.],\n",
            "        [360., 220., 424., 317.],\n",
            "        [167., 233., 185., 267.],\n",
            "        [550., 300., 585., 402.],\n",
            "        [405., 221., 443., 308.],\n",
            "        [448., 121., 462., 142.],\n",
            "        [477., 351., 636., 424.],\n",
            "        [231., 172., 267., 213.],\n",
            "        [332., 176., 368., 226.],\n",
            "        [412., 158., 475., 295.],\n",
            "        [358., 215., 374., 231.],\n",
            "        [349., 211., 362., 231.],\n",
            "        [357., 218., 422., 318.],\n",
            "        [550., 300., 586., 400.]])\n",
            " Print results\n",
            "tensor(4)\n",
            "image 1/1 /content/000000000139.jpg: 448x640 1 person, 1 bottle, 3 chairs, 2 potted plants, 3 dining tables, 2 tvs, 2 refrigerators, 1 clock, 4 vases, \n",
            "[0.887499988079071, 0.8215962648391724, 0.05624999850988388, 0.23474177718162537] 60\n",
            "[0.608593761920929, 0.6291079521179199, 0.1015625, 0.23474177718162537] 60\n",
            "[0.555468738079071, 0.5187793374061584, 0.02031249925494194, 0.04694835841655731] 75\n",
            "[0.5718749761581421, 0.5234741568565369, 0.02500000037252903, 0.03755868598818779] 75\n",
            "[0.6929687261581421, 0.5316901206970215, 0.09843750298023224, 0.32159623503685] 72\n",
            "[0.546875, 0.47183099389076233, 0.05624999850988388, 0.11737088859081268] 58\n",
            "[0.3890624940395355, 0.4518779218196869, 0.05624999850988388, 0.09624413400888443] 58\n",
            "[0.8695312738418579, 0.9096243977546692, 0.24843749403953552, 0.17136150598526] 60\n",
            "[0.7109375, 0.30868545174598694, 0.02187499962747097, 0.04929577559232712] 74\n",
            "[0.6625000238418579, 0.6208920478820801, 0.05937499925494194, 0.2042253464460373] 56\n",
            "[0.88671875, 0.8239436745643616, 0.0546875, 0.23943662643432617] 75\n",
            "[0.2750000059604645, 0.5868544578552246, 0.02812499925494194, 0.07981220632791519] 75\n",
            "[0.612500011920929, 0.6302816867828369, 0.10000000149011612, 0.22769953310489655] 56\n",
            "[0.887499988079071, 0.8215962648391724, 0.05624999850988388, 0.23004694283008575] 39\n",
            "[0.7523437738418579, 0.5387324094772339, 0.1015625, 0.2887323796749115] 72\n",
            "[0.9359375238418579, 0.5809859037399292, 0.12812499701976776, 0.1666666716337204] 62\n",
            "[0.686718761920929, 0.5328638553619385, 0.08281250298023224, 0.3380281627178192] 0\n",
            "[0.5054687261581421, 0.6291079521179199, 0.09531249850988388, 0.23474177718162537] 56\n",
            "[0.125, 0.5046948194503784, 0.23125000298023224, 0.23004694283008575] 62\n",
            "tensor(1.01037)\n",
            "tensor(1.03542)\n",
            "tensor(1.00959)\n",
            "tensor(0.99942)\n",
            "tensor(1.00796)\n",
            "tensor(1.00335)\n",
            "tensor(1.28844)\n",
            "tensor(1.00858)\n",
            "tensor(0.86293)\n",
            "tensor(0.92262)\n",
            "tensor(1.00293)\n",
            "tensor(1.00843)\n",
            "tensor(1.01640)\n",
            "tensor(1.01836)\n",
            "tensor(1.01693)\n",
            "tensor(1.02363)\n",
            "tensor(1.00384)\n",
            "tensor(1.12427)\n",
            "tensor(0.96724)\n",
            "predictions [tensor([[4.54711e+02, 1.42183e+02, 1.37548e+01,  ..., 5.19641e-05, 4.78500e-04, 5.84892e-05],\n",
            "        [3.50238e+02, 2.12142e+02, 3.32178e+01,  ..., 4.76685e-04, 1.86613e-04, 6.69654e-04],\n",
            "        [2.49539e+02, 2.03706e+02, 3.74948e+01,  ..., 6.94562e-04, 1.04249e-04, 3.52081e-04],\n",
            "        ...,\n",
            "        [3.23191e+02, 2.78497e+02, 6.13302e+01,  ..., 9.95533e-05, 5.18045e-05, 4.60034e-05],\n",
            "        [3.87519e+02, 2.79996e+02, 5.71495e+01,  ..., 3.73452e-05, 2.41177e-05, 2.67736e-05],\n",
            "        [4.39278e+02, 2.39511e+02, 5.44535e+01,  ..., 2.39333e-04, 1.18503e-04, 1.44748e-04]])]\n",
            "number of predictions is: 19\n",
            "predicted labels are: [74, 58, 58, 58, 56, 56, 75, 74, 75, 75, 56, 74, 58, 72, 0, 72, 56, 60, 0]\n",
            "true labels are: [60]\n",
            "60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 /content/000000000074.jpg: 448x640 5 persons, 1 bicycle, 1 dog, 277.0ms\n",
            "Speed: 0.9ms pre-process, 277.0ms inference, 10.6ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/yolov5/runs/detect/exp3\u001b[0m\n",
            "0 labels saved to /content/yolov5/runs/detect/exp3/labels\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape probs: torch.Size([7, 85])\n",
            "tensor(1.06985)\n",
            "tensor(1.00872)\n",
            "tensor(1.03392)\n",
            "tensor(0.97729)\n",
            "tensor(0.99991)\n",
            "tensor(0.91981)\n",
            "tensor(1.00646)\n",
            "Rescale boxes from img_size to im0 size\n",
            "tensor([[  0.,  14., 162., 309.],\n",
            "        [ 59., 278., 376., 382.],\n",
            "        [461., 106., 492., 148.],\n",
            "        [293.,  96., 316., 154.],\n",
            "        [323.,  96., 342., 151.],\n",
            "        [276., 107., 291., 149.],\n",
            "        [355.,  98., 372., 148.]])\n",
            " Print results\n",
            "tensor(1)\n",
            "image 1/1 /content/000000000074.jpg: 448x640 5 persons, 1 bicycle, 1 dog, \n",
            "[0.5679687261581421, 0.2887323796749115, 0.02656250074505806, 0.11737088859081268] 0\n",
            "[0.4429687559604645, 0.3004694879055023, 0.0234375, 0.09859155118465424] 0\n",
            "[0.51953125, 0.2899061143398285, 0.02968749962747097, 0.1291079819202423] 0\n",
            "[0.47578126192092896, 0.2934272289276123, 0.03593749925494194, 0.13615024089813232] 0\n",
            "[0.7445312738418579, 0.2981220781803131, 0.04843749850988388, 0.09859155118465424] 0\n",
            "[0.33984375, 0.7746478915214539, 0.49531251192092896, 0.24413146078586578] 16\n",
            "[0.12656250596046448, 0.3791079819202423, 0.25312501192092896, 0.6924882531166077] 1\n",
            "tensor(1.06985)\n",
            "tensor(1.00872)\n",
            "tensor(1.03392)\n",
            "tensor(0.97729)\n",
            "tensor(0.99991)\n",
            "tensor(0.91981)\n",
            "tensor(1.00646)\n",
            "predictions [tensor([[2.83944e+02, 1.36802e+02, 1.56550e+01, 4.17718e+01, 3.61514e-01, 8.51303e-01, 1.88617e-03, 1.05890e-03, 5.43768e-04, 3.65089e-05, 2.21813e-04, 6.31806e-05, 1.51897e-04, 8.81174e-05, 4.93820e-04, 1.19777e-04, 1.54737e-04, 6.76463e-04, 2.73850e-04, 1.02959e-04, 5.83177e-05, 1.72709e-04, 2.74863e-04, 4.61777e-05,\n",
            "         4.06470e-05, 6.37930e-05, 4.36545e-05, 4.94929e-05, 1.46382e-04, 8.75565e-02, 6.61546e-04, 1.16076e-01, 7.32391e-04, 7.96667e-04, 8.94952e-05, 2.52568e-04, 1.26332e-04, 9.96195e-05, 1.92196e-04, 6.54108e-05, 2.24234e-04, 1.92149e-04, 1.35394e-04, 2.59015e-04, 2.80690e-04, 7.70712e-05, 2.01500e-04, 4.99637e-05,\n",
            "         8.09248e-05, 5.45669e-05, 6.07253e-05, 1.57391e-04, 6.74952e-05, 4.93482e-05, 1.15811e-04, 4.27864e-05, 4.04399e-05, 5.13647e-05, 4.95783e-05, 4.17686e-05, 3.13459e-05, 4.67653e-04, 5.35298e-05, 1.61054e-04, 5.01883e-05, 1.60657e-04, 4.14698e-05, 7.85350e-05, 8.45439e-05, 2.67615e-05, 4.64915e-05, 3.42236e-05,\n",
            "         8.30807e-04, 4.31251e-05, 7.18240e-05, 9.53271e-05, 3.21116e-05, 5.07818e-05, 1.45689e-04, 9.67957e-05, 5.21852e-05, 4.27357e-05, 7.72509e-05, 7.34816e-05, 5.23862e-05],\n",
            "        [4.83252e+02, 1.36221e+02, 1.94363e+01, 4.00206e+01, 3.34884e-01, 9.64499e-01, 7.61401e-04, 4.71420e-04, 3.39936e-04, 4.90343e-05, 1.13080e-04, 7.18939e-05, 1.75780e-04, 1.00745e-04, 1.49545e-04, 1.41003e-04, 8.01296e-05, 1.90608e-04, 8.07884e-04, 1.28409e-04, 8.26833e-05, 2.95099e-04, 2.32006e-04, 5.53901e-05,\n",
            "         6.21965e-05, 1.01109e-04, 5.79744e-05, 5.66448e-05, 1.01515e-04, 7.24566e-03, 3.47647e-04, 2.25594e-02, 8.24433e-04, 5.25333e-04, 1.48030e-04, 2.57714e-04, 1.71167e-04, 2.01793e-04, 1.70246e-04, 1.30884e-04, 3.28634e-04, 3.77071e-04, 1.83436e-04, 3.21269e-04, 4.12257e-04, 1.10444e-04, 3.94689e-04, 7.70386e-05,\n",
            "         8.17507e-05, 1.16266e-04, 1.17680e-04, 1.26473e-04, 7.47189e-05, 6.75752e-05, 9.27757e-05, 5.20215e-05, 8.02519e-05, 8.74293e-05, 7.72283e-05, 7.26434e-05, 5.70386e-05, 8.47581e-04, 1.06682e-04, 1.98257e-04, 8.77282e-05, 2.29857e-04, 7.51058e-05, 9.01384e-05, 1.51511e-04, 4.81445e-05, 9.07381e-05, 5.36051e-05,\n",
            "         6.17993e-04, 6.40841e-05, 9.99092e-05, 9.34764e-05, 4.79217e-05, 8.68935e-05, 1.50164e-04, 1.45546e-04, 8.74156e-05, 5.45737e-05, 1.21361e-04, 7.73626e-05, 7.47079e-05],\n",
            "        [2.83919e+02, 1.36879e+02, 1.59253e+01, 4.45559e+01, 3.96024e-01, 8.98761e-01, 1.32924e-03, 8.14940e-04, 3.59007e-04, 2.74851e-05, 2.17549e-04, 5.99725e-05, 1.66951e-04, 8.93294e-05, 3.34123e-04, 7.90046e-05, 8.79833e-05, 4.13414e-04, 1.60085e-04, 7.48599e-05, 3.02838e-05, 1.15746e-04, 2.10467e-04, 3.26621e-05,\n",
            "         3.43941e-05, 4.59155e-05, 2.30552e-05, 3.30311e-05, 1.02292e-04, 5.59631e-02, 3.46837e-04, 7.00412e-02, 2.29590e-04, 5.61556e-04, 4.02541e-05, 2.00181e-04, 7.57630e-05, 4.94244e-05, 1.27818e-04, 3.25865e-05, 5.34399e-05, 1.01554e-04, 9.66461e-05, 1.04526e-04, 1.59325e-04, 4.33878e-05, 8.71242e-05, 2.87361e-05,\n",
            "         5.24864e-05, 3.51289e-05, 3.14202e-05, 7.85098e-05, 3.87355e-05, 2.35248e-05, 5.98060e-05, 3.12673e-05, 2.60529e-05, 2.60488e-05, 1.89081e-05, 2.90067e-05, 1.95280e-05, 3.39049e-04, 3.24521e-05, 1.16820e-04, 2.45947e-05, 7.91734e-05, 2.08521e-05, 5.08690e-05, 3.54770e-05, 1.95878e-05, 2.36062e-05, 1.90345e-05,\n",
            "         2.18779e-04, 2.34841e-05, 4.76587e-05, 7.26782e-05, 1.90528e-05, 2.74591e-05, 8.43469e-05, 5.33476e-05, 3.46161e-05, 2.42980e-05, 5.62535e-05, 4.78611e-05, 3.13166e-05],\n",
            "        [4.75832e+02, 1.37956e+02, 3.05771e+01, 4.39383e+01, 6.81964e-01, 9.53551e-01, 1.20149e-03, 3.33366e-04, 1.16169e-03, 2.25243e-05, 6.10398e-05, 2.81222e-05, 3.34567e-04, 9.76905e-05, 3.69899e-05, 1.20624e-04, 2.32615e-05, 6.37008e-05, 1.81710e-03, 5.31868e-05, 1.78958e-05, 2.25220e-04, 2.27880e-04, 1.64322e-05,\n",
            "         7.84848e-05, 6.82347e-05, 1.78650e-05, 3.51425e-05, 2.88151e-05, 4.31392e-03, 1.04668e-04, 8.82991e-03, 1.25949e-04, 3.72243e-04, 3.86661e-05, 7.34023e-05, 7.83814e-05, 4.34412e-05, 1.54720e-04, 4.13533e-05, 6.32965e-05, 1.80777e-04, 7.20589e-05, 9.74593e-05, 1.30906e-04, 2.50153e-05, 9.22236e-05, 4.22090e-05,\n",
            "         1.97248e-05, 3.15298e-05, 3.23764e-05, 2.52131e-05, 2.15674e-05, 3.56394e-05, 2.04295e-05, 2.21496e-05, 2.82423e-05, 2.90256e-05, 1.18520e-05, 1.76145e-05, 1.34427e-05, 1.33114e-03, 7.97217e-05, 1.11662e-04, 2.87315e-05, 1.58537e-04, 2.13304e-05, 2.69296e-05, 6.95595e-05, 1.76362e-05, 1.99555e-05, 3.47865e-05,\n",
            "         1.04307e-04, 2.32334e-05, 5.35117e-05, 4.00480e-05, 1.64806e-05, 3.07151e-05, 1.50166e-04, 3.76756e-05, 2.19838e-05, 1.58048e-05, 4.16642e-05, 2.29826e-05, 1.68331e-05],\n",
            "        [3.32979e+02, 1.33372e+02, 1.66596e+01, 5.17241e+01, 4.98485e-01, 9.81649e-01, 5.47233e-04, 2.57897e-04, 1.36026e-04, 2.78748e-05, 2.08021e-04, 5.31035e-05, 1.05600e-04, 1.27819e-04, 7.17449e-05, 4.96126e-05, 4.30027e-05, 7.76035e-05, 2.02212e-04, 6.05195e-05, 1.73549e-05, 1.07662e-04, 1.87212e-04, 2.25905e-05,\n",
            "         2.61371e-05, 4.62589e-05, 1.23161e-05, 2.29235e-05, 9.88939e-05, 3.74427e-03, 1.33411e-04, 9.45104e-03, 3.93572e-05, 1.66256e-04, 3.15900e-05, 1.46048e-04, 4.27725e-05, 3.88611e-05, 1.04523e-04, 2.16129e-05, 1.53917e-05, 4.36809e-05, 6.34763e-05, 6.41472e-05, 1.26324e-04, 2.67285e-05, 4.41960e-05, 2.75999e-05,\n",
            "         3.15190e-05, 4.13397e-05, 2.64785e-05, 3.15138e-05, 3.59759e-05, 1.82820e-05, 2.63171e-05, 1.23948e-05, 3.19151e-05, 2.06681e-05, 1.50128e-05, 2.19818e-05, 1.12353e-05, 2.24084e-04, 3.00375e-05, 8.88366e-05, 1.68782e-05, 6.63405e-05, 1.53431e-05, 4.88923e-05, 2.93807e-05, 1.89453e-05, 2.24116e-05, 1.06555e-05,\n",
            "         9.70099e-05, 1.61405e-05, 3.66322e-05, 4.26798e-05, 1.34693e-05, 3.28180e-05, 7.26786e-05, 4.55202e-05, 2.88751e-05, 2.74290e-05, 4.57138e-05, 3.37667e-05, 2.77198e-05],\n",
            "        [4.81524e+02, 1.37587e+02, 2.38293e+01, 4.28719e+01, 3.54739e-01, 8.50830e-01, 6.96713e-03, 5.59954e-04, 5.65257e-04, 4.82698e-05, 4.27175e-04, 1.07265e-04, 5.72533e-04, 2.46543e-04, 7.88757e-05, 1.36686e-04, 5.30034e-05, 1.04705e-04, 1.13688e-02, 8.54067e-05, 2.89998e-05, 2.71670e-04, 5.53284e-04, 3.88969e-05,\n",
            "         6.53040e-05, 9.23831e-05, 1.69524e-05, 1.06334e-04, 1.29913e-04, 6.94787e-03, 1.99255e-04, 2.23836e-02, 1.37965e-04, 1.08410e-03, 4.39176e-05, 3.30258e-04, 9.97810e-05, 1.00928e-04, 2.61935e-04, 9.91891e-05, 9.78763e-05, 3.07805e-04, 9.01006e-05, 7.78039e-04, 1.78248e-04, 3.89107e-05, 1.09773e-04, 1.02951e-04,\n",
            "         3.59577e-05, 3.69403e-05, 4.47158e-05, 4.66909e-05, 3.06876e-05, 2.81262e-05, 3.39113e-05, 2.67123e-05, 4.24249e-05, 4.31629e-05, 3.61270e-05, 2.45871e-05, 1.85180e-05, 1.02225e-02, 1.55727e-04, 2.51178e-04, 5.20781e-05, 4.14132e-04, 4.25246e-05, 6.21234e-05, 1.26786e-04, 2.15216e-05, 4.39437e-05, 8.76644e-05,\n",
            "         1.32126e-04, 2.69206e-05, 1.41604e-04, 5.31936e-05, 2.57091e-05, 4.48692e-05, 3.27067e-04, 7.61086e-05, 3.98026e-05, 5.24306e-05, 5.61657e-05, 3.33699e-05, 2.61271e-05],\n",
            "        [4.76272e+02, 1.38363e+02, 3.10578e+01, 4.16095e+01, 6.14410e-01, 9.80951e-01, 4.88161e-03, 4.50140e-04, 4.37847e-03, 7.60671e-05, 1.59371e-04, 1.39509e-04, 3.71293e-04, 2.45226e-04, 9.10127e-05, 2.71285e-04, 5.67166e-05, 1.28076e-04, 3.90487e-03, 1.05580e-04, 6.58241e-05, 1.76422e-04, 2.05930e-04, 7.46042e-05,\n",
            "         9.45493e-05, 8.67283e-05, 5.31064e-05, 6.36611e-05, 7.33486e-05, 1.26865e-03, 2.06631e-04, 1.57730e-03, 5.64385e-05, 2.43662e-04, 5.97771e-05, 9.87220e-05, 9.79597e-05, 8.77180e-05, 1.03518e-04, 8.46638e-05, 7.15086e-05, 1.02414e-04, 6.05964e-05, 7.97115e-05, 9.60643e-05, 9.15936e-05, 8.19056e-05, 9.96462e-05,\n",
            "         5.09540e-05, 6.13697e-05, 8.78302e-05, 5.82677e-05, 6.11593e-05, 4.21419e-05, 6.31819e-05, 5.95364e-05, 6.29205e-05, 5.68149e-05, 4.49175e-05, 5.62084e-05, 4.84307e-05, 2.33802e-03, 1.01632e-04, 1.65797e-04, 6.18350e-05, 1.94344e-04, 7.72459e-05, 7.24006e-05, 4.19537e-05, 6.80400e-05, 4.45217e-05, 4.87404e-05,\n",
            "         5.79502e-05, 4.85406e-05, 9.14766e-05, 6.19346e-05, 6.82604e-05, 5.62052e-05, 5.58268e-05, 7.10714e-05, 6.61062e-05, 5.64173e-05, 6.39407e-05, 7.21007e-05, 4.54170e-05]])]\n",
            "number of predictions is: 7\n",
            "predicted labels are: [0, 0, 0, 0, 0, 0, 0]\n",
            "true labels are: [         18           2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 /content/000000000009.jpg: 480x640 2 bowls, 2 broccolis, 1 dining table, 329.0ms\n",
            "Speed: 1.0ms pre-process, 329.0ms inference, 9.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/yolov5/runs/detect/exp3\u001b[0m\n",
            "0 labels saved to /content/yolov5/runs/detect/exp3/labels\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape probs: torch.Size([5, 85])\n",
            "tensor(1.01276)\n",
            "tensor(1.34580)\n",
            "tensor(1.01389)\n",
            "tensor(1.00977)\n",
            "tensor(1.02566)\n",
            "Rescale boxes from img_size to im0 size\n",
            "tensor([[339.,   0., 629., 239.],\n",
            "        [256., 227., 573., 480.],\n",
            "        [250., 222., 572., 477.],\n",
            "        [327., 411., 427., 480.],\n",
            "        [  0.,   6., 623., 480.]])\n",
            " Print results\n",
            "tensor(1)\n",
            "image 1/1 /content/000000000009.jpg: 480x640 2 bowls, 2 broccolis, 1 dining table, \n",
            "[0.4867187440395355, 0.5062500238418579, 0.973437488079071, 0.987500011920929] 60\n",
            "[0.589062511920929, 0.9281250238418579, 0.15625, 0.14374999701976776] 50\n",
            "[0.6421874761581421, 0.7281249761581421, 0.503125011920929, 0.53125] 45\n",
            "[0.647656261920929, 0.7364583611488342, 0.49531251192092896, 0.5270833373069763] 50\n",
            "[0.7562500238418579, 0.24895833432674408, 0.453125, 0.49791666865348816] 45\n",
            "tensor(1.01276)\n",
            "tensor(1.34580)\n",
            "tensor(1.01389)\n",
            "tensor(1.00977)\n",
            "tensor(1.02566)\n",
            "predictions [tensor([[3.74803e+02, 4.45548e+02, 9.62394e+01, 6.83098e+01, 2.61003e-01, 3.19408e-05, 1.47414e-04, 1.72741e-04, 3.20744e-04, 4.80515e-04, 2.52728e-05, 8.36080e-06, 5.56281e-04, 9.53580e-05, 9.10464e-05, 4.11259e-05, 2.23088e-05, 4.21998e-05, 3.28912e-05, 1.16177e-04, 1.87308e-04, 1.03436e-04, 1.91343e-05, 6.04147e-04,\n",
            "         1.48607e-03, 1.94808e-05, 2.73128e-04, 5.83400e-04, 1.79683e-04, 2.27860e-04, 5.27289e-05, 6.79770e-05, 6.62733e-04, 8.28912e-05, 6.17414e-05, 2.10001e-04, 8.48789e-04, 1.17970e-05, 2.85339e-04, 5.69810e-06, 8.99396e-05, 1.73083e-05, 7.65300e-06, 1.93279e-04, 1.84133e-04, 5.10929e-05, 2.41176e-04, 2.70308e-03,\n",
            "         4.98351e-04, 2.20273e-04, 8.33996e-04, 1.74380e-04, 6.34922e-04, 9.12072e-04, 1.25356e-04, 9.75424e-01, 1.61484e-03, 1.17529e-04, 1.49978e-03, 1.12143e-03, 1.02969e-02, 1.40617e-04, 6.19360e-05, 1.33480e-04, 8.54274e-05, 1.64399e-04, 8.89135e-05, 1.70401e-05, 3.99487e-04, 1.57485e-05, 3.84972e-05, 4.95502e-03,\n",
            "         1.05795e-04, 1.01396e-04, 2.06064e-04, 2.01243e-05, 1.40710e-04, 7.79429e-06, 2.18841e-04, 7.31812e-05, 1.27337e-04, 3.18643e-04, 1.32165e-04, 2.57242e-05, 6.65582e-05],\n",
            "        [4.12191e+02, 3.48751e+02, 3.15356e+02, 2.56974e+02, 4.68053e-01, 4.49789e-05, 3.25663e-05, 6.77110e-05, 5.62893e-05, 3.87033e-05, 4.49848e-05, 3.97531e-05, 5.04200e-05, 5.17305e-05, 4.73185e-05, 3.37831e-05, 4.25662e-05, 4.64663e-05, 4.68353e-05, 5.35411e-05, 1.39722e-04, 4.05251e-05, 3.63858e-05, 5.45029e-05,\n",
            "         6.20158e-05, 5.66490e-05, 6.60575e-05, 9.09976e-05, 5.18118e-05, 1.72302e-04, 9.16299e-05, 1.16560e-04, 6.57423e-05, 2.45635e-04, 1.08389e-04, 3.42413e-05, 5.32800e-05, 5.12135e-05, 4.26073e-05, 2.46786e-05, 4.85720e-05, 4.48928e-05, 3.50649e-05, 4.72466e-05, 1.20552e-04, 8.52772e-05, 1.18469e-03, 1.71277e-04,\n",
            "         1.63560e-04, 8.06443e-04, 5.15551e-01, 4.63990e-04, 9.29724e-04, 6.59213e-04, 7.60080e-04, 8.13810e-01, 2.07257e-03, 8.74355e-05, 2.05006e-04, 2.16847e-04, 2.31648e-03, 1.36208e-04, 5.21955e-05, 1.80016e-04, 4.03293e-05, 1.41216e-03, 7.70573e-05, 6.74586e-05, 1.88892e-04, 1.05525e-04, 4.86022e-05, 8.84337e-05,\n",
            "         6.98036e-05, 7.71635e-05, 1.60947e-04, 1.26772e-04, 3.07564e-04, 4.27174e-05, 1.22246e-04, 5.16209e-05, 1.43499e-04, 6.83856e-05, 6.13458e-05, 3.40664e-05, 5.71482e-05],\n",
            "        [3.76750e+02, 4.45833e+02, 9.97488e+01, 6.80672e+01, 2.57273e-01, 1.42691e-04, 1.13058e-04, 6.93292e-05, 1.74179e-04, 1.09777e-04, 9.36849e-05, 1.43535e-04, 7.25806e-05, 7.78902e-05, 1.38174e-04, 8.82862e-05, 1.04072e-04, 1.29902e-04, 1.09633e-04, 1.89956e-04, 2.88930e-04, 6.90188e-05, 8.48156e-05, 3.43694e-04,\n",
            "         1.04474e-04, 2.08140e-04, 1.84389e-04, 2.10626e-04, 1.17801e-04, 2.15288e-04, 1.53507e-04, 7.60151e-05, 1.78348e-04, 3.26376e-04, 8.24803e-05, 1.02236e-04, 1.07262e-04, 1.12530e-04, 9.63425e-05, 6.36940e-05, 1.01054e-04, 1.24519e-04, 1.02127e-04, 1.44303e-04, 1.53055e-04, 8.65342e-05, 2.11803e-04, 2.43639e-04,\n",
            "         2.16474e-04, 4.58690e-04, 1.04003e-03, 4.04758e-04, 5.61230e-04, 5.90734e-04, 2.82048e-04, 9.93789e-01, 9.89733e-04, 1.18431e-04, 7.26238e-04, 1.78936e-03, 5.11989e-04, 1.14933e-04, 1.06288e-04, 2.78140e-04, 7.30510e-05, 1.13341e-03, 1.48605e-04, 9.75263e-05, 7.26765e-05, 1.81532e-04, 1.73755e-04, 2.01253e-03,\n",
            "         1.34312e-04, 1.30232e-04, 3.42957e-04, 7.70904e-05, 1.24599e-04, 1.17058e-04, 1.23205e-04, 1.23037e-04, 3.49292e-04, 9.07529e-05, 1.44393e-04, 6.15148e-05, 1.48322e-04],\n",
            "        [3.77286e+02, 4.45725e+02, 1.00534e+02, 6.84171e+01, 2.61260e-01, 2.66487e-04, 1.15727e-04, 1.15210e-04, 2.23886e-04, 1.32202e-04, 1.12722e-04, 1.55941e-04, 1.11830e-04, 1.00914e-04, 1.65300e-04, 1.07371e-04, 1.28644e-04, 1.39435e-04, 1.45584e-04, 1.96785e-04, 2.91958e-04, 9.83239e-05, 1.28290e-04, 3.98336e-04,\n",
            "         1.58405e-04, 2.12006e-04, 2.08253e-04, 2.08332e-04, 1.62389e-04, 2.15010e-04, 1.95417e-04, 1.03686e-04, 1.89864e-04, 3.79095e-04, 1.04497e-04, 1.13191e-04, 1.13897e-04, 1.25389e-04, 1.16364e-04, 7.97337e-05, 1.16171e-04, 1.42210e-04, 1.14965e-04, 1.89802e-04, 1.77709e-04, 1.17047e-04, 2.45888e-04, 2.68041e-04,\n",
            "         2.35151e-04, 5.00738e-04, 1.53210e-03, 3.97650e-04, 4.42866e-04, 6.96730e-04, 3.02563e-04, 9.89741e-01, 1.06558e-03, 1.38086e-04, 5.96061e-04, 9.91658e-04, 4.43471e-04, 1.69356e-04, 1.34654e-04, 3.56037e-04, 9.93685e-05, 1.05797e-03, 1.89389e-04, 1.18000e-04, 9.27632e-05, 1.55881e-04, 1.34697e-04, 7.61335e-04,\n",
            "         1.43747e-04, 1.51514e-04, 4.31397e-04, 8.97583e-05, 1.55855e-04, 1.31638e-04, 1.45526e-04, 1.31648e-04, 3.51290e-04, 9.73829e-05, 1.89025e-04, 6.81806e-05, 1.40840e-04],\n",
            "        [4.72487e+02, 1.19475e+02, 3.15026e+02, 2.45028e+02, 3.54418e-01, 6.23405e-04, 1.17350e-04, 4.37711e-04, 1.68921e-04, 1.07805e-04, 1.91028e-04, 8.72447e-05, 2.24926e-04, 3.28698e-04, 1.63045e-04, 1.16991e-04, 1.24532e-04, 1.19610e-04, 1.33729e-04, 1.02699e-04, 2.14165e-04, 1.82842e-04, 1.10703e-04, 1.19493e-04,\n",
            "         3.43962e-04, 1.68665e-04, 1.33568e-04, 4.17009e-04, 1.24807e-04, 1.65115e-04, 2.59248e-04, 2.99880e-04, 1.08620e-04, 2.04473e-04, 3.38524e-04, 1.17884e-04, 1.29571e-04, 1.74983e-04, 1.61888e-04, 1.21158e-04, 1.39186e-04, 1.31712e-04, 1.23991e-04, 1.78399e-04, 7.46887e-04, 8.10698e-04, 1.84529e-02, 5.95060e-04,\n",
            "         3.19312e-04, 2.50841e-03, 9.61088e-01, 2.80221e-03, 2.35482e-03, 2.47652e-03, 1.10852e-02, 5.13890e-04, 3.71002e-03, 6.25495e-04, 5.11536e-04, 3.34009e-04, 7.76416e-04, 3.49171e-04, 4.20461e-04, 4.77210e-04, 8.51723e-05, 1.67043e-03, 2.55710e-04, 2.01427e-04, 3.47770e-04, 1.73804e-04, 1.37549e-04, 1.44917e-04,\n",
            "         1.99005e-04, 2.32228e-04, 3.73027e-04, 2.05507e-04, 7.33947e-04, 1.59859e-04, 8.05739e-04, 2.12656e-04, 6.23681e-04, 1.74886e-04, 1.53751e-04, 1.42115e-04, 1.46603e-04]])]\n",
            "number of predictions is: 5\n",
            "predicted labels are: [50, 50, 50, 50, 45]\n",
            "true labels are: [         45          45          50          45          49          49          49          49]\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "45\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "folder_path = \"/content/\"\n",
        "image_extension = \"*.jpg\"\n",
        "\n",
        "\n",
        "images = glob.glob(os.path.join(folder_path, image_extension))\n",
        "total_prediction=[]\n",
        "\n",
        "from models.yolo import DetectionModel\n",
        "\n",
        "hit_number=0\n",
        "print('number of images is:',len(images))\n",
        "for image_path in images:\n",
        "    total_prediction,xywh, predicted_class, txt_path,probs  = detection_probs(image_path)\n",
        "    if len(probs)==0:\n",
        "      continue\n",
        "    print('predictions',total_prediction)\n",
        "    text_file = os.path.splitext(image_path)[0] + '.txt'\n",
        "    real_labels = np.loadtxt(text_file, delimiter=' ')\n",
        "    if real_labels.ndim>1:\n",
        "        real_labels=real_labels[:,0]\n",
        "    else:\n",
        "        real_labels=[int(real_labels[0])]\n",
        "    number_of_predictions=len(total_prediction[0])\n",
        "    print('number of predictions is:',number_of_predictions)\n",
        "    predicted_labels=[]\n",
        "    for prediction in range(len(total_prediction[0])):\n",
        "        pred2=np.zeros((len(total_prediction[0]),80))\n",
        "        for num in range(len(total_prediction[0][prediction][5:])):\n",
        "            pred2[prediction][num]= total_prediction[0][prediction][5+num]\n",
        "        predicted_labels.append(np.argmax(pred2[prediction]).tolist())\n",
        "    print('predicted labels are:',predicted_labels)\n",
        "    print('true labels are:', real_labels)\n",
        "    for label in predicted_labels:\n",
        "        if label in real_labels:\n",
        "            print(label)\n",
        "\n",
        "        #real_labels = np.delete(real_labels, np.where(real_labels == label))\n",
        "        hit_number =hit_number+1\n",
        "    \"\"\"\n",
        "    for label in predicted_labels:\n",
        "    if label in real_labels:\n",
        "        idx = np.where(real_labels == label)[0][0]\n",
        "        real_labels = np.delete(real_labels, idx)\n",
        "        print(1)\n",
        "    \"\"\"\n",
        "    #print('Now real labels are:',real_labels)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4d6nG28ZRFxh"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOC/8epkB0QyOgGSEQh65fj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}