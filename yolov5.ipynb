{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lluisdn/TFM/blob/YOLO/yolov5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5J5ZrQcq3jb"
      },
      "source": [
        "#**Clone YOLOv5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDD4EEDlLZnT",
        "outputId": "10146619-0bb0-4b61-91cf-8f908093544a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['pip', 'install', '-r', 'requirements.txt'], returncode=0)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "\n",
        "# Clone YOLOv5 repository\n",
        "if not os.path.exists('yolov5'):\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/ultralytics/yolov5.git'])\n",
        "\n",
        "\n",
        "# Change working directory to yolov5 directory\n",
        "os.chdir('yolov5')\n",
        "\n",
        "# Install required packages\n",
        "subprocess.run(['pip', 'install', '-r', 'requirements.txt'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwmoUn8aC3RL",
        "outputId": "93efb5c2-eb72-4545-f0b6-270581ce1e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=/content/000000000009.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=True, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /usr/local/lib/python3.10/dist-packages/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-168-gec2b853 Python-3.10.11 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "[tensor(339., device='cuda:0'), tensor(0., device='cuda:0'), tensor(629., device='cuda:0'), tensor(239., device='cuda:0')]\n",
            "image 1/1 /content/000000000009.jpg: 480x640 1 bowl, 2 broccolis, 1 dining table, 43.4ms\n",
            "Speed: 0.5ms pre-process, 43.4ms inference, 86.9ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights yolov5s.pt --img-size 640 --conf 0.25 --source /content/000000000009.jpg --agnostic-nms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLRzIi5Pe1IN",
        "outputId": "3fe53938-4506-4576-846a-d62b6d75fdaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco128.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 🚀 v7.0-177-g89c3040 Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 140MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "\n",
            "Dataset not found ⚠️, missing paths ['/content/datasets/coco128/images/train2017']\n",
            "Downloading https://ultralytics.com/assets/coco128.zip to coco128.zip...\n",
            "100% 6.66M/6.66M [00:00<00:00, 88.1MB/s]\n",
            "Dataset download success ✅ (0.4s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 16.3MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<00:00, 1539.56it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco128/labels/train2017.cache\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:09<00:00,  2.29s/it]\n",
            "                   all        128        929      0.714      0.625      0.708      0.475\n",
            "Speed: 0.2ms pre-process, 11.9ms inference, 9.8ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python val.py --weights yolov5s.pt --data coco128.yaml --img 640 --half"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXnhdC7KoLKD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DEBUG:\n",
        "        import pdb; pdb.set_trace()\n",
        "        !pip install -Uqq ipdb\n",
        "        %pdb on\n",
        "\"\"\"\n",
        "\n",
        "print(compute_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IduLfUiMoa_j",
        "outputId": "542533d6-7518-4d5e-df23-76d49b40a1f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uqq ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnW8PdW8oz_Y",
        "outputId": "7c6c3d57-abde-4aa6-c673-984d4879c8ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ],
      "source": [
        "%pdb on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltJeJu8LL7rq",
        "outputId": "9823258f-68e7-4bcc-a399-20bf47873956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n",
            "3\n",
            "tensor(5.3644e-07, device='cuda:0', dtype=torch.float16)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load the tensor from the file\n",
        "loaded_tensor = torch.load('preds.pt')\n",
        "\n",
        "print(len(loaded_tensor[0]))\n",
        "print(len(loaded_tensor[1]))\n",
        "#print(loaded_tensor)\n",
        "probs_000=loaded_tensor[0][0][0][4]\n",
        "\n",
        "print(torch.cumsum(probs_000, dim=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il6himzAOYoQ",
        "outputId": "ee922af2-bf4f-4ebd-980d-c707181df7bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0000e+00, 2.0000e+01, 6.2863e-01, 6.2066e-01, 7.1479e-01, 6.8973e-01],\n",
            "        [0.0000e+00, 2.0000e+01, 3.3524e-01, 4.8392e-01, 6.3706e-01, 8.8064e-01],\n",
            "        [1.0000e+00, 3.0000e+00, 4.9752e-01, 5.1129e-01, 8.8155e-01, 9.0725e-01],\n",
            "        [1.0000e+00, 3.0000e+00, 2.5898e-01, 2.3106e-01, 4.4178e-01, 4.0463e-01],\n",
            "        [2.0000e+00, 7.1000e+01, 3.1872e-01, 5.9675e-01, 1.7953e-01, 2.2038e-02],\n",
            "        [2.0000e+00, 4.5000e+01, 3.2075e-01, 6.0817e-01, 1.9570e-01, 5.2571e-02],\n",
            "        [3.0000e+00, 0.0000e+00, 5.7278e-01, 7.3051e-01, 2.4257e-01, 4.1548e-01],\n",
            "        [3.0000e+00, 0.0000e+00, 8.5426e-01, 3.9677e-01, 1.4674e-01, 1.3973e-01],\n",
            "        [3.0000e+00, 3.8000e+01, 6.4742e-01, 6.9824e-01, 1.7349e-01, 6.0744e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 2.1156e-01, 3.8917e-01, 7.1480e-02, 3.3140e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 2.8865e-01, 3.8798e-01, 7.4967e-02, 3.7857e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 4.4502e-01, 3.8592e-01, 7.8125e-02, 3.8720e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 5.2211e-01, 5.0290e-01, 9.3026e-02, 4.2083e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 3.3683e-01, 4.3103e-01, 8.2451e-02, 4.5015e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 2.5990e-01, 4.2452e-01, 8.2993e-02, 3.8408e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 1.7582e-01, 4.2912e-01, 7.7171e-02, 3.5238e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 1.3387e-01, 3.9113e-01, 7.8520e-02, 3.3393e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 8.0101e-01, 5.1113e-01, 1.0117e-01, 6.9256e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 6.5063e-01, 4.5547e-01, 8.4753e-02, 3.2158e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 6.1362e-01, 5.0178e-01, 8.4507e-02, 4.0580e-02],\n",
            "        [3.0000e+00, 5.6000e+01, 5.1035e-01, 4.2158e-01, 1.0931e-01, 1.1519e-01],\n",
            "        [3.0000e+00, 5.6000e+01, 3.6374e-01, 4.0071e-01, 8.0115e-02, 6.1801e-02],\n",
            "        [4.0000e+00, 2.7000e+01, 3.9373e-01, 6.1740e-01, 1.0137e-01, 3.5686e-01],\n",
            "        [4.0000e+00, 0.0000e+00, 4.9908e-01, 5.5778e-01, 8.4684e-01, 8.1542e-01],\n",
            "        [5.0000e+00, 0.0000e+00, 4.3534e-01, 5.7602e-01, 2.1242e-01, 5.5948e-01],\n",
            "        [5.0000e+00, 3.0000e+00, 5.3498e-01, 7.5423e-01, 4.8398e-01, 4.2952e-01],\n",
            "        [5.0000e+00, 2.6000e+01, 6.2566e-01, 5.4417e-01, 2.2138e-01, 1.6396e-01],\n",
            "        [6.0000e+00, 1.7000e+01, 5.7853e-01, 6.3100e-01, 2.7473e-01, 3.5130e-01],\n",
            "        [6.0000e+00, 1.7000e+01, 3.5897e-01, 6.4103e-01, 1.7635e-01, 3.0133e-01],\n",
            "        [6.0000e+00, 0.0000e+00, 5.9635e-01, 5.8943e-01, 1.3839e-01, 1.3912e-01],\n",
            "        [6.0000e+00, 0.0000e+00, 4.0812e-01, 5.8150e-01, 1.1989e-01, 1.1987e-01],\n",
            "        [6.0000e+00, 0.0000e+00, 3.6267e-01, 6.9237e-01, 2.5153e-02, 6.3943e-02],\n",
            "        [6.0000e+00, 5.8000e+01, 5.9836e-01, 8.8058e-01, 1.4923e-01, 8.9962e-02],\n",
            "        [6.0000e+00, 0.0000e+00, 7.0915e-01, 6.8498e-01, 2.2920e-02, 5.2248e-02],\n",
            "        [6.0000e+00, 0.0000e+00, 5.0860e-01, 6.8300e-01, 1.5084e-02, 4.4590e-02],\n",
            "        [6.0000e+00, 0.0000e+00, 8.4477e-01, 6.6764e-01, 3.1178e-02, 1.7562e-02],\n",
            "        [7.0000e+00, 2.5000e+01, 4.8082e-01, 4.1859e-01, 7.5276e-01, 6.4040e-01],\n",
            "        [7.0000e+00, 0.0000e+00, 6.3550e-01, 6.1233e-01, 5.1087e-01, 6.9225e-01],\n",
            "        [8.0000e+00, 6.1000e+01, 4.5614e-01, 5.5577e-01, 4.3623e-01, 7.9795e-01],\n",
            "        [9.0000e+00, 7.2000e+01, 6.9988e-01, 4.6148e-01, 3.8970e-01, 8.6677e-01],\n",
            "        [9.0000e+00, 5.3000e+01, 4.4643e-01, 5.0827e-01, 3.6447e-01, 2.1613e-01],\n",
            "        [1.0000e+01, 1.3000e+01, 7.8557e-01, 3.8916e-01, 2.1834e-01, 4.5406e-01],\n",
            "        [1.0000e+01, 7.7000e+01, 4.8137e-01, 5.2461e-01, 7.4749e-01, 8.8175e-01],\n",
            "        [1.0000e+01, 5.6000e+01, 7.8617e-01, 3.7208e-01, 2.0973e-01, 4.5684e-01],\n",
            "        [1.0000e+01, 0.0000e+00, 6.6986e-01, 4.9382e-01, 4.4566e-01, 9.3507e-01],\n",
            "        [1.1000e+01, 2.0000e+00, 3.3641e-01, 7.1812e-01, 2.9007e-01, 2.3455e-01],\n",
            "        [1.1000e+01, 7.0000e+00, 2.9429e-01, 5.9452e-01, 2.0490e-01, 8.8497e-02],\n",
            "        [1.1000e+01, 1.1000e+01, 3.2623e-01, 4.1693e-01, 6.0888e-02, 1.1186e-01],\n",
            "        [1.1000e+01, 7.4000e+01, 4.1643e-01, 1.9992e-01, 2.5378e-01, 2.2665e-01],\n",
            "        [1.2000e+01, 0.0000e+00, 8.4521e-01, 1.5498e-01, 5.0263e-02, 7.9286e-02],\n",
            "        [1.2000e+01, 6.0000e+00, 6.7332e-01, 5.0342e-01, 4.4283e-01, 9.4554e-01],\n",
            "        [1.2000e+01, 1.1000e+01, 4.8630e-01, 5.3998e-01, 6.2697e-02, 5.2664e-02],\n",
            "        [1.3000e+01, 0.0000e+00, 4.6202e-01, 5.9239e-01, 4.6447e-01, 7.6760e-01],\n",
            "        [1.3000e+01, 6.5000e+01, 6.9259e-01, 5.8389e-01, 7.3174e-02, 2.7664e-02],\n",
            "        [1.3000e+01, 7.3000e+01, 5.4070e-01, 6.7443e-01, 1.6661e-02, 1.0448e-01],\n",
            "        [1.3000e+01, 7.3000e+01, 2.2691e-01, 2.8932e-01, 1.1875e-02, 2.3378e-02],\n",
            "        [1.3000e+01, 7.3000e+01, 1.6528e-01, 2.8976e-01, 1.5230e-02, 4.2976e-02],\n",
            "        [1.3000e+01, 7.3000e+01, 1.5655e-01, 2.8998e-01, 1.3816e-02, 4.3616e-02],\n",
            "        [1.3000e+01, 7.3000e+01, 1.9003e-01, 2.9121e-01, 4.6727e-02, 4.4940e-02],\n",
            "        [1.3000e+01, 7.3000e+01, 2.0403e-01, 2.0519e-01, 1.5503e-01, 6.8378e-02],\n",
            "        [1.3000e+01, 7.3000e+01, 2.5249e-01, 1.3987e-01, 2.2714e-02, 4.2902e-02],\n",
            "        [1.3000e+01, 5.7000e+01, 1.8025e-01, 8.6175e-01, 1.4452e-01, 1.9987e-01],\n",
            "        [1.3000e+01, 5.8000e+01, 8.3628e-01, 4.9841e-01, 9.6085e-02, 1.2616e-01],\n",
            "        [1.3000e+01, 5.8000e+01, 7.7248e-01, 4.2625e-01, 1.0281e-01, 1.4688e-01],\n",
            "        [1.3000e+01, 5.8000e+01, 8.5086e-01, 3.6553e-01, 8.7763e-02, 1.5457e-01],\n",
            "        [1.4000e+01, 0.0000e+00, 5.7468e-01, 5.1823e-01, 6.4012e-01, 9.1592e-01],\n",
            "        [1.4000e+01, 5.0000e+01, 6.9938e-01, 5.8784e-01, 3.9071e-01, 3.6302e-01],\n",
            "        [1.5000e+01, 3.9000e+01, 1.8922e-01, 9.1205e-01, 4.9161e-02, 1.2827e-01],\n",
            "        [1.5000e+01, 5.7000e+01, 7.5668e-01, 6.1746e-01, 2.7500e-01, 4.4051e-01],\n",
            "        [1.5000e+01, 5.7000e+01, 2.2350e-01, 6.3156e-01, 2.3539e-01, 3.8754e-01],\n",
            "        [1.5000e+01, 0.0000e+00, 4.8835e-01, 4.9949e-01, 3.2966e-01, 9.3180e-01],\n",
            "        [1.5000e+01, 0.0000e+00, 2.7676e-01, 6.7496e-01, 3.3826e-01, 5.1793e-01],\n",
            "        [1.5000e+01, 0.0000e+00, 6.7975e-01, 6.0660e-01, 1.7623e-01, 3.8266e-01],\n",
            "        [1.5000e+01, 4.1000e+01, 3.1757e-01, 9.4292e-01, 8.3421e-02, 6.6533e-02],\n",
            "        [1.5000e+01, 4.1000e+01, 1.4104e-01, 9.4382e-01, 7.1283e-02, 6.4747e-02],\n",
            "        [1.5000e+01, 6.5000e+01, 3.3414e-01, 5.1902e-01, 7.2566e-02, 3.2976e-02],\n",
            "        [1.5000e+01, 2.6000e+01, 7.2466e-01, 7.7734e-01, 1.0742e-01, 1.0933e-01],\n",
            "        [1.5000e+01, 0.0000e+00, 1.4239e-01, 7.2981e-01, 7.4260e-02, 3.0265e-01],\n",
            "        [1.5000e+01, 0.0000e+00, 8.2526e-01, 6.9797e-01, 1.3896e-01, 3.4243e-01],\n",
            "        [1.5000e+01, 5.8000e+01, 3.4502e-01, 2.7933e-01, 1.8066e-01, 2.9940e-01],\n",
            "        [1.5000e+01, 5.8000e+01, 3.6734e-01, 5.9880e-01, 8.1875e-02, 1.0754e-01],\n",
            "        [1.6000e+01, 3.2000e+01, 4.8505e-01, 4.1529e-01, 7.8026e-02, 7.3973e-02],\n",
            "        [1.6000e+01, 0.0000e+00, 5.6599e-01, 3.8658e-01, 3.7766e-01, 4.0524e-01],\n",
            "        [1.6000e+01, 0.0000e+00, 4.0900e-01, 2.2629e-01, 8.8388e-02, 1.6299e-01],\n",
            "        [1.6000e+01, 0.0000e+00, 2.5765e-01, 3.4551e-01, 9.3783e-02, 5.3021e-02],\n",
            "        [1.6000e+01, 0.0000e+00, 3.6076e-01, 3.0609e-01, 6.5033e-02, 1.2269e-01],\n",
            "        [1.6000e+01, 0.0000e+00, 5.8099e-01, 1.9163e-01, 6.0954e-02, 1.2210e-01],\n",
            "        [1.6000e+01, 0.0000e+00, 8.3996e-01, 3.0355e-01, 7.2549e-02, 1.4689e-01],\n",
            "        [1.6000e+01, 0.0000e+00, 6.3724e-01, 3.8273e-01, 1.2595e-01, 3.5451e-01],\n",
            "        [1.6000e+01, 0.0000e+00, 7.3225e-01, 3.0110e-01, 6.1151e-02, 1.4595e-01],\n",
            "        [1.6000e+01, 0.0000e+00, 2.6217e-01, 2.7646e-01, 4.7763e-02, 9.8601e-02],\n",
            "        [1.6000e+01, 0.0000e+00, 2.2771e-01, 2.7668e-01, 3.0987e-02, 9.4881e-02],\n",
            "        [1.6000e+01, 0.0000e+00, 4.7385e-01, 2.2484e-01, 3.2763e-02, 4.4137e-02],\n",
            "        [1.6000e+01, 0.0000e+00, 1.2069e-01, 2.9806e-01, 3.0855e-02, 1.3832e-01],\n",
            "        [1.7000e+01, 3.9000e+01, 6.5912e-01, 3.4505e-01, 2.6327e-01, 3.7778e-01],\n",
            "        [1.7000e+01, 6.0000e+01, 5.0000e-01, 5.2897e-01, 7.8947e-01, 3.3664e-01],\n",
            "        [1.7000e+01, 4.6000e+01, 5.0413e-01, 7.2422e-01, 4.4842e-01, 2.8615e-01],\n",
            "        [1.7000e+01, 4.8000e+01, 5.2691e-01, 7.8002e-01, 5.9796e-01, 3.9235e-01],\n",
            "        [1.8000e+01, 5.7000e+01, 2.8331e-01, 8.1077e-01, 3.0016e-01, 3.2481e-01],\n",
            "        [1.8000e+01, 7.4000e+01, 4.9507e-01, 4.5520e-01, 4.6480e-02, 4.1771e-02],\n",
            "        [1.8000e+01, 7.3000e+01, 4.2226e-01, 8.4403e-01, 6.9326e-02, 2.6146e-02],\n",
            "        [1.8000e+01, 7.3000e+01, 3.4456e-01, 8.4048e-01, 6.7484e-02, 3.5878e-02],\n",
            "        [1.8000e+01, 6.4000e+01, 3.0921e-01, 7.2246e-01, 1.9803e-02, 1.4330e-02],\n",
            "        [1.8000e+01, 6.5000e+01, 7.5108e-01, 7.6866e-01, 3.4951e-02, 1.0595e-02],\n",
            "        [1.8000e+01, 7.3000e+01, 3.6110e-01, 8.1650e-01, 7.4046e-02, 3.1399e-02],\n",
            "        [1.8000e+01, 6.5000e+01, 4.1445e-01, 8.1813e-01, 5.1332e-02, 9.8662e-03],\n",
            "        [1.8000e+01, 6.5000e+01, 3.8539e-01, 8.3780e-01, 1.7072e-02, 8.3482e-03],\n",
            "        [1.8000e+01, 7.3000e+01, 4.2354e-01, 7.9481e-01, 5.3026e-02, 2.4777e-02],\n",
            "        [1.8000e+01, 6.3000e+01, 3.6830e-01, 7.3589e-01, 6.4260e-02, 2.8750e-02],\n",
            "        [1.9000e+01, 3.9000e+01, 3.1970e-01, 3.1913e-01, 1.3553e-01, 4.6391e-01],\n",
            "        [1.9000e+01, 5.6000e+01, 1.9738e-01, 3.0028e-01, 9.7961e-02, 1.2850e-01],\n",
            "        [1.9000e+01, 5.6000e+01, 6.2225e-01, 3.3859e-01, 4.5946e-01, 4.3010e-01],\n",
            "        [1.9000e+01, 6.0000e+01, 4.9725e-01, 6.5013e-01, 6.9845e-01, 6.2521e-01],\n",
            "        [1.9000e+01, 4.0000e+01, 4.9160e-01, 4.1570e-01, 1.4516e-01, 3.1878e-01],\n",
            "        [2.0000e+01, 2.2000e+01, 4.5563e-01, 7.3481e-01, 5.7415e-01, 4.8275e-01],\n",
            "        [2.0000e+01, 2.2000e+01, 4.4041e-01, 4.1453e-01, 4.5003e-01, 2.0710e-01],\n",
            "        [2.0000e+01, 2.2000e+01, 6.2427e-01, 2.1056e-01, 1.6314e-01, 9.4836e-02],\n",
            "        [2.1000e+01, 2.3000e+01, 6.1130e-01, 5.8775e-01, 4.7546e-01, 7.4263e-01],\n",
            "        [2.1000e+01, 2.3000e+01, 4.2386e-01, 5.5362e-01, 3.8406e-01, 8.4513e-01],\n",
            "        [2.2000e+01, 0.0000e+00, 4.9825e-01, 5.1355e-01, 2.3900e-01, 8.0621e-01],\n",
            "        [2.2000e+01, 0.0000e+00, 3.7117e-01, 6.6062e-01, 2.1194e-01, 5.1528e-01],\n",
            "        [2.2000e+01, 2.8000e+01, 7.3789e-01, 7.5268e-01, 2.2651e-01, 3.3003e-01],\n",
            "        [2.3000e+01, 0.0000e+00, 5.0593e-01, 4.9356e-01, 6.9044e-01, 9.3094e-01],\n",
            "        [2.3000e+01, 5.4000e+01, 5.9529e-01, 4.3093e-01, 1.7012e-01, 1.0793e-01],\n",
            "        [2.4000e+01, 0.0000e+00, 4.8238e-01, 5.7384e-01, 4.7781e-01, 3.1247e-01],\n",
            "        [2.4000e+01, 3.6000e+01, 5.3621e-01, 7.2836e-01, 2.6505e-01, 8.0461e-02],\n",
            "        [2.4000e+01, 0.0000e+00, 4.1245e-01, 7.0990e-02, 3.5658e-02, 4.5789e-02],\n",
            "        [2.4000e+01, 0.0000e+00, 7.7850e-01, 1.0943e-01, 1.4531e-01, 1.7083e-01],\n",
            "        [2.4000e+01, 0.0000e+00, 4.5705e-01, 6.7232e-02, 3.4556e-02, 3.5952e-02],\n",
            "        [2.4000e+01, 0.0000e+00, 2.9839e-01, 1.0234e-01, 4.2039e-02, 4.2530e-02],\n",
            "        [2.5000e+01, 3.0000e+00, 4.7576e-01, 6.3860e-01, 5.9572e-01, 5.1866e-01],\n",
            "        [2.5000e+01, 0.0000e+00, 5.1591e-01, 4.4298e-01, 3.7722e-01, 6.1107e-01],\n",
            "        [2.5000e+01, 0.0000e+00, 4.1477e-01, 3.1541e-01, 4.4786e-01, 2.9183e-01],\n",
            "        [2.6000e+01, 0.0000e+00, 5.0311e-01, 5.1391e-01, 6.9431e-01, 8.1754e-01],\n",
            "        [2.6000e+01, 3.5000e+01, 2.6287e-01, 5.3839e-01, 1.8646e-01, 1.3583e-01],\n",
            "        [2.7000e+01, 8.0000e+00, 4.9731e-01, 6.2341e-01, 5.1676e-01, 1.2866e-01],\n",
            "        [2.7000e+01, 8.0000e+00, 6.2947e-01, 5.9366e-01, 3.2757e-01, 6.8393e-02],\n",
            "        [2.7000e+01, 8.0000e+00, 6.9380e-01, 5.7275e-01, 2.5835e-01, 4.4062e-02],\n",
            "        [2.7000e+01, 8.0000e+00, 6.5325e-01, 5.6850e-01, 1.4107e-01, 2.4226e-02],\n",
            "        [2.7000e+01, 8.0000e+00, 6.1852e-01, 5.6112e-01, 4.0625e-02, 1.6399e-02],\n",
            "        [2.8000e+01, 7.9000e+01, 4.4659e-01, 3.4998e-01, 1.2929e-01, 1.7662e-01],\n",
            "        [2.8000e+01, 7.9000e+01, 5.2676e-01, 2.7453e-01, 1.2895e-01, 3.3034e-01],\n",
            "        [2.8000e+01, 7.9000e+01, 3.2482e-01, 2.8105e-01, 1.3720e-01, 2.7180e-01],\n",
            "        [2.8000e+01, 4.1000e+01, 5.1008e-01, 5.3109e-01, 5.0865e-01, 7.8406e-01],\n",
            "        [2.9000e+01, 5.6000e+01, 4.3820e-01, 6.3818e-01, 2.5785e-01, 9.4167e-02],\n",
            "        [2.9000e+01, 5.6000e+01, 2.3509e-01, 9.2933e-01, 1.1811e-01, 9.2173e-02],\n",
            "        [2.9000e+01, 0.0000e+00, 2.8977e-01, 4.0917e-01, 2.4951e-01, 6.7720e-01],\n",
            "        [2.9000e+01, 4.1000e+01, 3.1590e-01, 7.5121e-01, 8.4868e-02, 9.0967e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 7.4681e-01, 2.7565e-01, 3.5658e-02, 2.7917e-02],\n",
            "        [2.9000e+01, 4.3000e+01, 6.1414e-01, 6.2911e-01, 7.7648e-02, 1.2952e-01],\n",
            "        [2.9000e+01, 5.5000e+01, 6.0615e-01, 7.6645e-01, 4.2816e-01, 2.2257e-01],\n",
            "        [2.9000e+01, 0.0000e+00, 7.8075e-01, 4.1758e-01, 1.2271e-01, 4.8847e-01],\n",
            "        [2.9000e+01, 6.0000e+01, 5.1594e-01, 7.9237e-01, 6.5234e-01, 3.6765e-01],\n",
            "        [2.9000e+01, 0.0000e+00, 5.6238e-01, 3.7801e-01, 3.2171e-01, 5.7571e-01],\n",
            "        [2.9000e+01, 4.1000e+01, 6.4361e-01, 1.8562e-01, 4.1957e-02, 3.9836e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 7.4707e-01, 1.4130e-01, 3.8553e-02, 3.2812e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 7.4257e-01, 1.8838e-01, 3.9638e-02, 3.7202e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 7.9725e-01, 1.8507e-01, 2.9013e-02, 3.4673e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 4.9632e-01, 2.4884e-01, 3.2993e-02, 3.2202e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 4.6701e-01, 2.4635e-01, 3.8766e-02, 3.6920e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 4.6160e-01, 2.0199e-01, 3.2253e-02, 3.8140e-02],\n",
            "        [2.9000e+01, 4.1000e+01, 4.0452e-01, 2.4615e-01, 3.0855e-02, 3.3140e-02],\n",
            "        [3.0000e+01, 5.9000e+01, 5.3075e-01, 6.5638e-01, 5.3061e-01, 5.9284e-01],\n",
            "        [3.1000e+01, 3.8000e+01, 3.3818e-01, 6.5074e-01, 6.9030e-02, 6.6086e-02],\n",
            "        [3.1000e+01, 0.0000e+00, 3.0589e-01, 6.3132e-01, 2.0224e-01, 4.4884e-01]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Load the tensor from the file\n",
        "targets = torch.load('targets.pt')\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bB6_QXcorb4"
      },
      "source": [
        "#**CODE**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijosIxS7CczS"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mAM9ZnFrUnv"
      },
      "source": [
        "#*Non_max_supression2*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9cUckPhTeOX"
      },
      "outputs": [],
      "source": [
        "from utils.general import (xywh2xyxy, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
        "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)#,non_max_suppression2)\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "def non_max_suppression2(\n",
        "        prediction,\n",
        "        conf_thres=0.25,\n",
        "        iou_thres=0.45,\n",
        "        classes=None,\n",
        "        agnostic=False,\n",
        "        multi_label=False,\n",
        "        labels=(),\n",
        "        max_det=300,\n",
        "        nm=0,  # number of masks\n",
        "):\n",
        "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
        "\n",
        "    Returns:\n",
        "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
        "    \"\"\"\n",
        "\n",
        "    # Checks\n",
        "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "    if isinstance(prediction, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
        "        prediction = prediction[0]  # select only inference output\n",
        "\n",
        "    device = prediction.device\n",
        "    mps = 'mps' in device.type  # Apple MPS\n",
        "    if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\n",
        "        prediction = prediction.cpu()\n",
        "    bs = prediction.shape[0]  # batch size\n",
        "    nc = prediction.shape[2] - nm - 5  # number of classes\n",
        "    xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "    # Settings\n",
        "    # min_wh = 2  # (pixels) minimum box width and height\n",
        "    max_wh = 7680  # (pixels) maximum box width and height\n",
        "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
        "    redundant = True  # require redundant detections\n",
        "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "    merge = False  # use merge-NMS\n",
        "\n",
        "    #t = time.time()\n",
        "    mi = 5 + nc  # mask start index\n",
        "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
        "    output2 = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
        "    for xi, x in enumerate(prediction):  # image index, image inference\n",
        "        # Apply constraints\n",
        "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "        x = x[xc[xi]]  # confidence\n",
        "        x_info=x.clone()\n",
        "\n",
        "        # Cat apriori labels if autolabelling\n",
        "        if labels and len(labels[xi]):\n",
        "            lb = labels[xi]\n",
        "            v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n",
        "            v[:, :4] = lb[:, 1:5]  # box\n",
        "            v[:, 4] = 1.0  # conf\n",
        "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
        "            x = torch.cat((x, v), 0)\n",
        "\n",
        "        # If none remain process next image\n",
        "        if not x.shape[0]:\n",
        "            continue\n",
        "\n",
        "        # Compute conf\n",
        "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "        # Box/Mask\n",
        "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
        "        mask = x[:, mi:]  # zero columns if no masks\n",
        "\n",
        "        # Detections matrix nx6 (xyxy, conf, cls)\n",
        "        if multi_label:\n",
        "            i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
        "            x = torch.cat((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
        "        else:  # best class only\n",
        "            conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
        "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
        "\n",
        "        # Filter by class\n",
        "        if classes is not None:\n",
        "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "        # Apply finite constraint\n",
        "        # if not torch.isfinite(x).all():\n",
        "        #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "        # Check shape\n",
        "        n = x.shape[0]  # number of boxes\n",
        "        if not n:  # no boxes\n",
        "            continue\n",
        "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
        "\n",
        "        # Batched NMS\n",
        "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "        i = i[:max_det]  # limit detections\n",
        "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "            weights = iou * scores[None]  # box weights\n",
        "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "            if redundant:\n",
        "                i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "        output[xi] = x[i]\n",
        "        output2[xi]= x_info[i]\n",
        "        if mps:\n",
        "            output[xi] = output[xi].to(device)\n",
        "            output2[xi] = output2[xi].to(device)\n",
        "\n",
        "        #if (time.time() - t) > time_limit:\n",
        "        #    LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
        "        #    break  # time limit exceeded\n",
        "\n",
        "    return output2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAzlgKVzrCSg"
      },
      "source": [
        "# *Imports and parameters*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxQg_k96rs57",
        "outputId": "ffcb94f0-0e24-4270-cc91-37d1f702c326"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 v7.0-168-gec2b853 Python-3.10.11 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.lib.display import exists\n",
        "import argparse\n",
        "import os\n",
        "import platform\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
        "from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
        "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "\n",
        "weights=['yolov5s.pt']\n",
        "half=False\n",
        "device=''\n",
        "device = select_device(device)\n",
        "dnn=False\n",
        "data = 'data/coco128.yaml'\n",
        "imgsz=(640, 640)\n",
        "source= '/content/000000000009.jpg'#'/content/image.jpg'\n",
        "vid_stride=1\n",
        "visualize=False\n",
        "augment=False\n",
        "conf_thres=0.25\n",
        "iou_thres=0.25\n",
        "classes=None\n",
        "agnostic_nms=False\n",
        "max_det=1000\n",
        "project='/content/yolov5/runs/detect'\n",
        "name='exp'\n",
        "exist_ok=False\n",
        "save_txt=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtLa8xXUrMv9"
      },
      "source": [
        "#*Run code*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNOg4MeJnq9u",
        "outputId": "553e2b73-f927-4960-e0d8-b2b9a48fd87b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n"
          ]
        }
      ],
      "source": [
        "save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)\n",
        "(save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model = DetectMultiBackend(weights , device=device, dnn=dnn, data=data, fp16=half)\n",
        "stride, names, pt = model.stride, model.names, model.pt\n",
        "imgsz = check_img_size(imgsz, s=stride)\n",
        "\n",
        "# Dataloader\n",
        "bs = 1  # batch_size\n",
        "dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
        "vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "\n",
        "# Run inference\n",
        "model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
        "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())\n",
        "for path, im, im0s, vid_cap, s in dataset:\n",
        "    with dt[0]:\n",
        "        im = torch.from_numpy(im).to(model.device)\n",
        "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "  # Inference\n",
        "    with dt[1]:\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize)\n",
        "        pred2 = pred.copy()\n",
        "\n",
        "    # NMS\n",
        "    with dt[2]:\n",
        "        probs = non_max_suppression2(pred2, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxTt84r9zJ97"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsMJyRblAO1i"
      },
      "source": [
        "#**Code for probabilitties**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rscMzH6yvOEN"
      },
      "outputs": [],
      "source": [
        "source= '/content/image.jpg'\n",
        "def detection_probs(source, visualize=False):\n",
        "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        with dt[0]:\n",
        "            im = torch.from_numpy(im).to(model.device)\n",
        "            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "            if len(im.shape) == 3:\n",
        "                im = im[None]  # expand for batch dim\n",
        "      # Inference\n",
        "        with dt[1]:\n",
        "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "            pred = model(im, augment=augment, visualize=visualize)\n",
        "            pred2 = pred.copy()\n",
        "\n",
        "        # NMS\n",
        "        with dt[2]:\n",
        "            probs = non_max_suppression2(pred2, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "    return probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCMxdV-QzGjF",
        "outputId": "3ecff88b-6483-45a0-bef2-08cab5f2eb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of images is: 0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "folder_path = \"/content/\"\n",
        "image_extension = \"*.jpg\"\n",
        "\n",
        "\n",
        "images = glob.glob(os.path.join(folder_path, image_extension))\n",
        "total_prediction=[]\n",
        "\n",
        "\n",
        "hit_number=0\n",
        "print('number of images is:',len(images))\n",
        "for image_path in images:\n",
        "    total_prediction=detection_probs(image_path)\n",
        "    print('predictions',total_prediction)\n",
        "    text_file = os.path.splitext(image_path)[0] + '.txt'\n",
        "    real_labels = np.loadtxt(text_file, delimiter=' ')\n",
        "    if real_labels.ndim>1:\n",
        "        real_labels=real_labels[:,0]\n",
        "    else:\n",
        "        real_labels=[int(real_labels[0])]\n",
        "    number_of_predictions=len(total_prediction[0])\n",
        "    print('number of predictions is:',number_of_predictions)\n",
        "    predicted_labels=[]\n",
        "    for prediction in range(len(total_prediction[0])):\n",
        "        pred2=np.zeros((len(total_prediction[0]),80))\n",
        "        for num in range(len(total_prediction[0][prediction][5:])):\n",
        "            pred2[prediction][num]= total_prediction[0][prediction][5+num]\n",
        "        predicted_labels.append(np.argmax(pred2[prediction]).tolist())\n",
        "    print('predicted labels are:',predicted_labels)\n",
        "    print('real labels are:', real_labels)\n",
        "    for label in predicted_labels:\n",
        "        if label in real_labels:\n",
        "            print(label)\n",
        "\n",
        "        #real_labels = np.delete(real_labels, np.where(real_labels == label))\n",
        "        hit_number =hit_number+1\n",
        "    \"\"\"\n",
        "    for label in predicted_labels:\n",
        "    if label in real_labels:\n",
        "        idx = np.where(real_labels == label)[0][0]\n",
        "        real_labels = np.delete(real_labels, idx)\n",
        "        print(1)\n",
        "    \"\"\"\n",
        "    #print('Now real labels are:',real_labels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKwtbHkuXvhJ"
      },
      "source": [
        "#**Funcitons**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpIFdUX6YC0v"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) between two bounding boxes.\n",
        "    Each box should be in the format (x1, y1, x2, y2).\n",
        "    \"\"\"\n",
        "    # Calculate the coordinates of the intersection rectangle\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # If the boxes do not overlap, return IoU of 0\n",
        "    if x1 >= x2 or y1 >= y2:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate the area of intersection rectangle\n",
        "    intersection_area = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "    # Calculate the areas of both bounding boxes\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    # Calculate the IoU\n",
        "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_Kgv9l1Z2kC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  total_prediction[image_num][predictions_num]\n",
        "    predictions_num[0] = x\n",
        "    predictions_num[1] = y\n",
        "    predictions_num[2] = lenght x\n",
        "    predictions_num[3] = lenght y\n",
        "    predictions_num[4] = probability of image\n",
        "    predictions_num[5-->84] = probabilities by class\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK5hYTcAz3Dx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2D67XZOAXUt"
      },
      "source": [
        "#**Previous code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "zMXpnFxAZ05t",
        "outputId": "3455cdb5-7b34-45a4-c768-993eab81bdb4"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c788d7a1cf80>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/000000000009.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0mline_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /content/000000000009.txt not found."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "A = np.loadtxt(\"/content/000000000009.txt\", delimiter=' ')\n",
        "print(A[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv-jqY_PjnNI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "json_file_path = '/content/yolov5/runs/val/exp10/yolov5s_predictions.json'\n",
        "\n",
        "data = read_json_file(json_file_path)\n",
        "\n",
        "# Now you can work with the data obtained from the JSON file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE79uVmwj1kd",
        "outputId": "dcafede1-5780-433e-d6c8-6491405fd385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'image_id': 298251, 'category_id': 24, 'bbox': [70.0, 91.0, 66.875, 43.5], 'score': 0.91064}\n"
          ]
        }
      ],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N0MWwnQ77bY"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights yolov5s.pt --img-size 640 --conf 0.25 --source /content/plane_people.jpg --agnostic-nms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gLi0c372wqa"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights yolov5s.pt --img-size 640 --conf 0.25 --source /content/image.jpg --agnostic-nms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhrWd61QrWFi"
      },
      "outputs": [],
      "source": [
        "# Run object detection on an image file\n",
        "subprocess.run(['python', 'detect.py', '--weights', 'yolov5s.pt', '--img-size', '640', '--conf', '0.25', '--source', '/content/guernica.jpg'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywDoH4CFru8h"
      },
      "outputs": [],
      "source": [
        "# Run object detection on an image file\n",
        "subprocess.run(['python', 'detect.py', '--weights', 'yolov5s.pt', '--img-size', '640', '--conf', '0.25', '--source', '/content/poter_poster.jpg'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxzB_zZhDgyP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Set image path\n",
        "img_path = '/content/image.jpg'\n",
        "\n",
        "# Detect objects in image\n",
        "results = model(img_path)\n",
        "\n",
        "# Get class names and probabilities for each detection\n",
        "classes = results.pred[0][:, -1].numpy()\n",
        "probs = results.pred[0][:, 4].numpy()\n",
        "\n",
        "# Get unique class names\n",
        "class_names = model.module.names if hasattr(model, 'module') else model.names\n",
        "\n",
        "# Create dictionary to store class probabilities\n",
        "class_probs = {}\n",
        "\n",
        "# Loop through each unique class and get the top 10 probabilities\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_idxs = np.where(classes == i)[0]\n",
        "    class_probs_temp = probs[class_idxs]\n",
        "    top10_idxs = np.argsort(class_probs_temp)[::-1][:10]\n",
        "    top10_probs = class_probs_temp[top10_idxs]\n",
        "    class_probs[class_name] = top10_probs.tolist()\n",
        "\n",
        "# Print class names and top 10 probabilities\n",
        "for class_name, top10_probs in class_probs.items():\n",
        "    print(f\"{class_name}: {top10_probs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuzzQUrJsFUr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RutZredCChoD"
      },
      "source": [
        "# **val code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TUQ9d60ClQS"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 🚀 by Ultralytics, AGPL-3.0 license\n",
        "\"\"\"\n",
        "Validate a trained YOLOv5 detection model on a detection dataset\n",
        "\n",
        "Usage:\n",
        "    $ python val.py --weights yolov5s.pt --data coco128.yaml --img 640\n",
        "\n",
        "Usage - formats:\n",
        "    $ python val.py --weights yolov5s.pt                 # PyTorch\n",
        "                              yolov5s.torchscript        # TorchScript\n",
        "                              yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
        "                              yolov5s_openvino_model     # OpenVINO\n",
        "                              yolov5s.engine             # TensorRT\n",
        "                              yolov5s.mlmodel            # CoreML (macOS-only)\n",
        "                              yolov5s_saved_model        # TensorFlow SavedModel\n",
        "                              yolov5s.pb                 # TensorFlow GraphDef\n",
        "                              yolov5s.tflite             # TensorFlow Lite\n",
        "                              yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
        "                              yolov5s_paddle_model       # PaddlePaddle\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "FILE = Path(__file__).resolve()\n",
        "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.callbacks import Callbacks\n",
        "from utils.dataloaders import create_dataloader\n",
        "from utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size, check_requirements,\n",
        "                           check_yaml, coco80_to_coco91_class, colorstr, increment_path, non_max_suppression,\n",
        "                           print_args, scale_boxes, xywh2xyxy, xyxy2xywh)\n",
        "from utils.metrics import ConfusionMatrix, ap_per_class, box_iou\n",
        "from utils.plots import output_to_target, plot_images, plot_val_study\n",
        "from utils.torch_utils import select_device, smart_inference_mode\n",
        "\n",
        "\n",
        "def save_one_txt(predn, save_conf, shape, file):\n",
        "    # Save one txt result\n",
        "    gn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "    for *xyxy, conf, cls in predn.tolist():\n",
        "        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "        with open(file, 'a') as f:\n",
        "            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "\n",
        "def save_one_json(predn, jdict, path, class_map):\n",
        "    # Save one JSON result {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}\n",
        "    image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
        "    box = xyxy2xywh(predn[:, :4])  # xywh\n",
        "    box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
        "    for p, b in zip(predn.tolist(), box.tolist()):\n",
        "        jdict.append({\n",
        "            'image_id': image_id,\n",
        "            'category_id': class_map[int(p[5])],\n",
        "            'bbox': [round(x, 3) for x in b],\n",
        "            'score': round(p[4], 5)})\n",
        "\n",
        "\n",
        "def process_batch(detections, labels, iouv):\n",
        "    \"\"\"\n",
        "    Return correct prediction matrix\n",
        "    Arguments:\n",
        "        detections (array[N, 6]), x1, y1, x2, y2, conf, class\n",
        "        labels (array[M, 5]), class, x1, y1, x2, y2\n",
        "    Returns:\n",
        "        correct (array[N, 10]), for 10 IoU levels\n",
        "    \"\"\"\n",
        "    correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool)\n",
        "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
        "    correct_class = labels[:, 0:1] == detections[:, 5]\n",
        "    for i in range(len(iouv)):\n",
        "        x = torch.where((iou >= iouv[i]) & correct_class)  # IoU > threshold and classes match\n",
        "        if x[0].shape[0]:\n",
        "            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n",
        "            if x[0].shape[0] > 1:\n",
        "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
        "                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
        "                # matches = matches[matches[:, 2].argsort()[::-1]]\n",
        "                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
        "            correct[matches[:, 1].astype(int), i] = True\n",
        "    return torch.tensor(correct, dtype=torch.bool, device=iouv.device)\n",
        "\n",
        "\n",
        "@smart_inference_mode()\n",
        "def run(\n",
        "        data,\n",
        "        weights=None,  # model.pt path(s)\n",
        "        batch_size=32,  # batch size\n",
        "        imgsz=640,  # inference size (pixels)\n",
        "        conf_thres=0.001,  # confidence threshold\n",
        "        iou_thres=0.6,  # NMS IoU threshold\n",
        "        max_det=300,  # maximum detections per image\n",
        "        task='val',  # train, val, test, speed or study\n",
        "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        workers=8,  # max dataloader workers (per RANK in DDP mode)\n",
        "        single_cls=False,  # treat as single-class dataset\n",
        "        augment=False,  # augmented inference\n",
        "        verbose=False,  # verbose output\n",
        "        save_txt=False,  # save results to *.txt\n",
        "        save_hybrid=False,  # save label+prediction hybrid results to *.txt\n",
        "        save_conf=False,  # save confidences in --save-txt labels\n",
        "        save_json=False,  # save a COCO-JSON results file\n",
        "        project=ROOT / 'runs/val',  # save to project/name\n",
        "        name='exp',  # save to project/name\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        half=True,  # use FP16 half-precision inference\n",
        "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
        "        model=None,\n",
        "        dataloader=None,\n",
        "        save_dir=Path(''),\n",
        "        plots=True,\n",
        "        callbacks=Callbacks(),\n",
        "        compute_loss=None,\n",
        "):\n",
        "    # Initialize/load model and set device\n",
        "    training = model is not None\n",
        "    if training:  # called by train.py\n",
        "        device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model\n",
        "        half &= device.type != 'cpu'  # half precision only supported on CUDA\n",
        "        model.half() if half else model.float()\n",
        "    else:  # called directly\n",
        "        device = select_device(device, batch_size=batch_size)\n",
        "\n",
        "        # Directories\n",
        "        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
        "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "        # Load model\n",
        "        model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
        "        stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\n",
        "        imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "        half = model.fp16  # FP16 supported on limited backends with CUDA\n",
        "        if engine:\n",
        "            batch_size = model.batch_size\n",
        "        else:\n",
        "            device = model.device\n",
        "            if not (pt or jit):\n",
        "                batch_size = 1  # export.py models default to batch-size 1\n",
        "                LOGGER.info(f'Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')\n",
        "\n",
        "        # Data\n",
        "        data = check_dataset(data)  # check\n",
        "\n",
        "    # Configure\n",
        "    model.eval()\n",
        "    cuda = device.type != 'cpu'\n",
        "    is_coco = isinstance(data.get('val'), str) and data['val'].endswith(f'coco{os.sep}val2017.txt')  # COCO dataset\n",
        "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
        "    iouv = torch.linspace(0.5, 0.95, 10, device=device)  # iou vector for mAP@0.5:0.95\n",
        "    niou = iouv.numel()\n",
        "\n",
        "    # Dataloader\n",
        "    if not training:\n",
        "        if pt and not single_cls:  # check --weights are trained on --data\n",
        "            ncm = model.model.nc\n",
        "            assert ncm == nc, f'{weights} ({ncm} classes) trained on different --data than what you passed ({nc} ' \\\n",
        "                              f'classes). Pass correct combination of --weights and --data that are trained together.'\n",
        "        model.warmup(imgsz=(1 if pt else batch_size, 3, imgsz, imgsz))  # warmup\n",
        "        pad, rect = (0.0, False) if task == 'speed' else (0.5, pt)  # square inference for benchmarks\n",
        "        task = task if task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
        "        dataloader = create_dataloader(data[task],\n",
        "                                       imgsz,\n",
        "                                       batch_size,\n",
        "                                       stride,\n",
        "                                       single_cls,\n",
        "                                       pad=pad,\n",
        "                                       rect=rect,\n",
        "                                       workers=workers,\n",
        "                                       prefix=colorstr(f'{task}: '))[0]\n",
        "\n",
        "    seen = 0\n",
        "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
        "    names = model.names if hasattr(model, 'names') else model.module.names  # get class names\n",
        "    if isinstance(names, (list, tuple)):  # old format\n",
        "        names = dict(enumerate(names))\n",
        "    class_map = coco80_to_coco91_class() if is_coco else list(range(1000))\n",
        "    s = ('%22s' + '%11s' * 6) % ('Class', 'Images', 'Instances', 'P', 'R', 'mAP50', 'mAP50-95')\n",
        "    tp, fp, p, r, f1, mp, mr, map50, ap50, map = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "    dt = Profile(), Profile(), Profile()  # profiling times\n",
        "    loss = torch.zeros(3, device=device)\n",
        "    jdict, stats, ap, ap_class = [], [], [], []\n",
        "    callbacks.run('on_val_start')\n",
        "    pbar = tqdm(dataloader, desc=s, bar_format=TQDM_BAR_FORMAT)  # progress bar\n",
        "    for batch_i, (im, targets, paths, shapes) in enumerate(pbar):\n",
        "        callbacks.run('on_val_batch_start')\n",
        "        with dt[0]:\n",
        "            if cuda:\n",
        "                im = im.to(device, non_blocking=True)\n",
        "                targets = targets.to(device)\n",
        "            im = im.half() if half else im.float()  # uint8 to fp16/32\n",
        "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "            nb, _, height, width = im.shape  # batch size, channels, height, width\n",
        "\n",
        "        # Inference\n",
        "        with dt[1]:\n",
        "            preds, train_out = model(im) if compute_loss else (model(im, augment=augment), None)\n",
        "\n",
        "        # Loss\n",
        "        if compute_loss:\n",
        "            loss += compute_loss(train_out, targets)[1]  # box, obj, cls\n",
        "\n",
        "        # NMS\n",
        "        targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels\n",
        "        lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
        "        with dt[2]:\n",
        "            preds = non_max_suppression(preds,\n",
        "                                        conf_thres,\n",
        "                                        iou_thres,\n",
        "                                        labels=lb,\n",
        "                                        multi_label=True,\n",
        "                                        agnostic=single_cls,\n",
        "                                        max_det=max_det)\n",
        "\n",
        "        # Metrics\n",
        "        for si, pred in enumerate(preds):\n",
        "            labels = targets[targets[:, 0] == si, 1:]\n",
        "            nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions\n",
        "            path, shape = Path(paths[si]), shapes[si][0]\n",
        "            correct = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init\n",
        "            seen += 1\n",
        "\n",
        "            if npr == 0:\n",
        "                if nl:\n",
        "                    stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))\n",
        "                    if plots:\n",
        "                        confusion_matrix.process_batch(detections=None, labels=labels[:, 0])\n",
        "                continue\n",
        "\n",
        "            # Predictions\n",
        "            if single_cls:\n",
        "                pred[:, 5] = 0\n",
        "            predn = pred.clone()\n",
        "            scale_boxes(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
        "\n",
        "            # Evaluate\n",
        "            if nl:\n",
        "                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
        "                scale_boxes(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
        "                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
        "                correct = process_batch(predn, labelsn, iouv)\n",
        "                if plots:\n",
        "                    confusion_matrix.process_batch(predn, labelsn)\n",
        "            stats.append((correct, pred[:, 4], pred[:, 5], labels[:, 0]))  # (correct, conf, pcls, tcls)\n",
        "\n",
        "            # Save/log\n",
        "            if save_txt:\n",
        "                save_one_txt(predn, save_conf, shape, file=save_dir / 'labels' / f'{path.stem}.txt')\n",
        "            if save_json:\n",
        "                save_one_json(predn, jdict, path, class_map)  # append to COCO-JSON dictionary\n",
        "            callbacks.run('on_val_image_end', pred, predn, path, names, im[si])\n",
        "\n",
        "        # Plot images\n",
        "        if plots and batch_i < 3:\n",
        "            plot_images(im, targets, paths, save_dir / f'val_batch{batch_i}_labels.jpg', names)  # labels\n",
        "            plot_images(im, output_to_target(preds), paths, save_dir / f'val_batch{batch_i}_pred.jpg', names)  # pred\n",
        "\n",
        "        callbacks.run('on_val_batch_end', batch_i, im, targets, paths, shapes, preds)\n",
        "\n",
        "    # Compute metrics\n",
        "    stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]  # to numpy\n",
        "    if len(stats) and stats[0].any():\n",
        "        tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
        "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
        "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
        "    nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class\n",
        "\n",
        "    # Print results\n",
        "    pf = '%22s' + '%11i' * 2 + '%11.3g' * 4  # print format\n",
        "    LOGGER.info(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
        "    if nt.sum() == 0:\n",
        "        LOGGER.warning(f'WARNING ⚠️ no labels found in {task} set, can not compute metrics without labels')\n",
        "\n",
        "    # Print results per class\n",
        "    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
        "        for i, c in enumerate(ap_class):\n",
        "            LOGGER.info(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
        "\n",
        "    # Print speeds\n",
        "    t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image\n",
        "    if not training:\n",
        "        shape = (batch_size, 3, imgsz, imgsz)\n",
        "        LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}' % t)\n",
        "\n",
        "    # Plots\n",
        "    if plots:\n",
        "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
        "        callbacks.run('on_val_end', nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix)\n",
        "\n",
        "    # Save JSON\n",
        "    if save_json and len(jdict):\n",
        "        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n",
        "        anno_json = str(Path('../datasets/coco/annotations/instances_val2017.json'))  # annotations\n",
        "        pred_json = str(save_dir / f'{w}_predictions.json')  # predictions\n",
        "        LOGGER.info(f'\\nEvaluating pycocotools mAP... saving {pred_json}...')\n",
        "        with open(pred_json, 'w') as f:\n",
        "            json.dump(jdict, f)\n",
        "\n",
        "        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
        "            check_requirements('pycocotools>=2.0.6')\n",
        "            from pycocotools.coco import COCO\n",
        "            from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "            anno = COCO(anno_json)  # init annotations api\n",
        "            pred = anno.loadRes(pred_json)  # init predictions api\n",
        "            eval = COCOeval(anno, pred, 'bbox')\n",
        "            if is_coco:\n",
        "                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.im_files]  # image IDs to evaluate\n",
        "            eval.evaluate()\n",
        "            eval.accumulate()\n",
        "            eval.summarize()\n",
        "            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'pycocotools unable to run: {e}')\n",
        "\n",
        "    # Return results\n",
        "    model.float()  # for training\n",
        "    if not training:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
        "    maps = np.zeros(nc) + map\n",
        "    for i, c in enumerate(ap_class):\n",
        "        maps[c] = ap[i]\n",
        "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, help='batch size')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')\n",
        "    parser.add_argument('--max-det', type=int, default=300, help='maximum detections per image')\n",
        "    parser.add_argument('--task', default='val', help='train, val, test, speed or study')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\n",
        "    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--save-json', action='store_true', help='save a COCO-JSON results file')\n",
        "    parser.add_argument('--project', default=ROOT / 'runs/val', help='save to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
        "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
        "    opt = parser.parse_args()\n",
        "    opt.data = check_yaml(opt.data)  # check YAML\n",
        "    opt.save_json |= opt.data.endswith('coco.yaml')\n",
        "    opt.save_txt |= opt.save_hybrid\n",
        "    print_args(vars(opt))\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n",
        "\n",
        "    if opt.task in ('train', 'val', 'test'):  # run normally\n",
        "        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466\n",
        "            LOGGER.info(f'WARNING ⚠️ confidence threshold {opt.conf_thres} > 0.001 produces invalid results')\n",
        "        if opt.save_hybrid:\n",
        "            LOGGER.info('WARNING ⚠️ --save-hybrid will return high mAP from hybrid labels, not from predictions alone')\n",
        "        run(**vars(opt))\n",
        "\n",
        "    else:\n",
        "        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]\n",
        "        opt.half = torch.cuda.is_available() and opt.device != 'cpu'  # FP16 for fastest results\n",
        "        if opt.task == 'speed':  # speed benchmarks\n",
        "            # python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...\n",
        "            opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False\n",
        "            for opt.weights in weights:\n",
        "                run(**vars(opt), plots=False)\n",
        "\n",
        "        elif opt.task == 'study':  # speed vs mAP benchmarks\n",
        "            # python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n.pt yolov5s.pt...\n",
        "            for opt.weights in weights:\n",
        "                f = f'study_{Path(opt.data).stem}_{Path(opt.weights).stem}.txt'  # filename to save to\n",
        "                x, y = list(range(256, 1536 + 128, 128)), []  # x axis (image sizes), y axis\n",
        "                for opt.imgsz in x:  # img-size\n",
        "                    LOGGER.info(f'\\nRunning {f} --imgsz {opt.imgsz}...')\n",
        "                    r, _, t = run(**vars(opt), plots=False)\n",
        "                    y.append(r + t)  # results and times\n",
        "                np.savetxt(f, y, fmt='%10.4g')  # save\n",
        "            subprocess.run(['zip', '-r', 'study.zip', 'study_*.txt'])\n",
        "            plot_val_study(x=x)  # plot\n",
        "        else:\n",
        "            raise NotImplementedError(f'--task {opt.task} not in (\"train\", \"val\", \"test\", \"speed\", \"study\")')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    opt = parse_opt()\n",
        "    main(opt)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KAzlgKVzrCSg",
        "QtLa8xXUrMv9"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPVk/n9gLpaA3j3u0NP4+oN",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}